{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Reinforcement Learning *via* the Arcade Learning Environment and OpenAI's Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/!\\ Please run the cell below /!\\\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "JUPYTER_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "This notebook requires :\n",
    "- python 3.X with numpy, matplotlib\n",
    "- [tensorflow 1.3](https://www.tensorflow.org/install/)\n",
    "- [OpenAI's Gym](https://github.com/openai/gym)\n",
    "- [OpenAI's Baselines](https://github.com/openai/baselines) for their PER Implementation (to run the Rainbow algorithm) + their own dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "* Introduction [$\\hookrightarrow$](#Introduction)\n",
    "* A3C [$\\hookrightarrow$](#Asynchronous-Advantage-Actor-Critic-(A3C%29)\n",
    "* DDPG [$\\hookrightarrow$](#Deep-Deterministic-Policy-Gradient-(DDPG%29)\n",
    "* Rainbow DQN [$\\hookrightarrow$](#Rainbow-DQN)\n",
    "* Conclusion [$\\hookrightarrow$](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Background\n",
    "A quick recap on reinforcement learning and the notations we will adopt here : we address the problem of an **agent** learning to act in an **environment** in order to maximize a **reward** signal.\n",
    "\n",
    "\n",
    "At each time step, the environment provides an observation $s_t \\in S$, the agent responds by selecting an action $a_t \\in A$ and then the environment provides a reward $r_{t+1}$ and the next state $s_{t+1}$ with a discount factor $\\gamma \\in [0, 1]$.\n",
    "We assume that this system is a Markov Decision Process e.g. the state of the system at the time step $t+1$ only depends on the state and the action chose at the time step $t$.\n",
    "\n",
    "\n",
    "The goal of RL is to find a policy $\\pi$ (e.g. a probability distribution over actions for each state) that maximize the expected discounted return $G_t = \\sum_{k=0}^{+\\infty}{\\gamma^k R_{t+k+1}}$ over an episode in the environment.\n",
    "\n",
    "To do so, we learn an estimate of the **Q-Value function** $Q_\\pi(s_t, a_t) = E_\\pi[G_t|s_t, a_t]$ which is equal to the expected reward an agent will receive starting from a state $s_t$ and action $a_t$ and then acting under policy $\\pi$. Once this Q-Value function is known, an optimal policy is trivial : $\\pi(s_t) = max_{a \\in A}{Q(s_t, a)}$.\n",
    "\n",
    "Let's also define the **Value function** $V_\\pi(s_t) = E_\\pi[G_t|s_t]$ the expected discounted reward obtained following a policy $\\pi$ starting from a state $s_t$.\n",
    "\n",
    "\n",
    "*For more details, see the [wikipedia page on MDPs][wikiMDP].*\n",
    "\n",
    "\n",
    "\n",
    "### DQN Algorithm\n",
    "Theorically, some methods exist to compute the Q-Value function, such as tabular Q-Learning. The idea is to start from a random guess of the Q-Value of each pair (state, action) and iteratively apply [Bellman Equation][BellEq] until it converges toward the real value of the function *(see [this post by Arthur Juliani][JulianiPost] to get an example)*.\n",
    "\n",
    "\n",
    "But this method is only appliable on very few problems with tiny state and action spaces, and usual problems generally have nearly infinite large state space, and even if the idea is not bad, the tabular approach cannot work on them.\n",
    "\n",
    "\n",
    "Deep Reinforcement Learning is an efficient method to solve such problems. The idea is to approximate the Q-Value function with a deep neural network trained by gradient descent to then derive a well performing policy from it.\n",
    "It was successfully implemented for the first time in 2013 [[Mnih et al][DQN]] as DQN by combining the use of convolutional nets to efficiently processes the raw frames fed as input to the network and of a replay memory buffer not to learn only on immediate rewards.\n",
    "\n",
    "\n",
    "The optimization by gradient descent is realized wrt to the loss on a randomly chosen time step picked from the replay memory $$\\left(R_t + \\gamma_{t+1} max_aQ_\\bar{\\theta}(s_{t+1}, a) - Q\\theta(s_t, a_t)\\right)^2$$ with $Q_\\bar{\\theta}$ the *online network* (the network used to select the action) and $Q_\\theta$ the *target network* (a copy of the network periodically updated to stabilize the learning).\n",
    "\n",
    "\n",
    "DQN was the first concrete example of successful reinforcement learning algorithm, and it opened the way to many researches and improvements since 2015. In this notebook, I will present three modified versions of DQN that has lead to improvements or that extended the application domain :\n",
    "* A3C : an asynchronous version that allows parallelism\n",
    "* DDPG : a continuous version\n",
    "* Rainbow DQN : an improved version of DQN, one of the most efficient algorithm of Deep RL today\n",
    "\n",
    "\n",
    "[wikiMDP]: https://en.wikipedia.org/wiki/Markov_decision_process\n",
    "[BellEq]: https://en.wikipedia.org/wiki/Bellman_equation#Example\n",
    "[JulianiPost]: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "[DQN]: https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Advantage Actor-Critic (A3C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept\n",
    "Presented in 2016 [[Mnih et al. again](https://arxiv.org/pdf/1602.01783.pdf)], A3C is quite different from regular DQN : it's an Actor-Critic. First, let's debunk the name of the algorithm to understand it better :\n",
    "+ **Asynchronous :** Asynchronous algorithm is a great family of algos designed to run on parallel architecture to speed the learning up. The main idea is instead of having only one agent interacting with an environment, an asynchronous algo creates a global network and many workers on parallel threads with each their own environment and their own neural network. Then, during an episode, each agent copies the weights of the global network, interacts with it's own environment and apply a gradient descent on the weights of the global network.\n",
    "\n",
    " This has two main advantages :\n",
    "  - the parallelism of today hardware allows to run tens or even hundreds of workers in the same time on different threads, and so to maximize the work done in a given time\n",
    "  - each worker being independent of the others, they collect a lot of various experience and assure a better exploration of the state space, removing the necessity to have a replay memory buffer and reducing the bias\n",
    "\n",
    "\n",
    "+ **Actor-Critic :** Instead of just estimating the Q-Value function and then inducing a policy by acting greedily with respect to the action Q-Values, in Actor-Critic algorithms, the network estimate both the Value $V$ function and a policy $\\pi$. The value estimator is called the **critic** and the policy estimator the **actor**.\n",
    "\n",
    "\n",
    "\n",
    "+ **Advantage :** We define advantage as the difference between the Q-Value and the Value of a given state and action $A(s, a) = Q(s, a) - V(s)$ : it represents how much better an action is than expected. During a gradient descent, the updates usually use discounted reward to tell the quality of a taken action, but one way to be more efficient is instead to use an advantage estimate to get how much better it is than on average.\n",
    "\n",
    " We can estimate this advantage quite easily because $V(s_t)$ is the output of our actor-critic network and $Q(s_t, a_t)$ can be estimated by the discounted reward $R_t = \\sum_{k=0}^{+\\infty}{\\gamma^k r_{t+k}}$ : $$A_{est}(s_t, a_t) = R_t - V(s_t)$$\n",
    " \n",
    "![](./Images/A3C.png \"Architecture of an A3C Network\")\n",
    "<center>*Image from [Arthur Juliani's blog](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2)*</center>\n",
    "\n",
    "Additionnaly, to help the network to understand time dependencies, we add a recurrent layer made of LSTM cells (see [Chris Olah's blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) for more details).\n",
    "\n",
    "### Implementation\n",
    "*This code can be found in ./BlogFiles/Code/A3C/*\n",
    "\n",
    "Let's now see an actual implementation of the A3C algorithm applied to a simple Atari 2600 game : Pong.\n",
    "First, the general outline of the code architecture :\n",
    "- **Network.py** : defines a class that builds a neural network with convolution and LSTM layers\n",
    "- **Agent.py** : defines a worker that contains a local network and a local environment and that can interact within this environment to get experiences and train the global network\n",
    "- **main.py** : the main program that creates a global network and many workers, and run them on different threads\n",
    "- **Environment.py** : wrapper for [ALE games](https://github.com/mgbellemare/Arcade-Learning-Environment)\n",
    "- **settings.py** : contains every important parameter that can be modified in the algorithm\n",
    "- **RMSPropApplier.py** : an object based on [RMS Prop Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer) from Tensorflow but that can be shared between the different threads like in the original A3C paper. The code for this object has been taken on [miyosuda's github](https://github.com/miyosuda/async_deep_reinforce/blob/master/rmsprop_applier.py)\n",
    "- **Displayer.py**, **Saver.py**, **eval.py** : defines visualisation, saving and testing tools\n",
    "\n",
    "\n",
    "Now, let's analyze more precisely the main lines of the code.\n",
    "First of all, I'll explain quickly the Environment wrapper because the interface is pretty much the same for the three kinds of algorithms we'll analyze here.<br/>\n",
    "So the file defines a class Environment with four main methods :\n",
    "- reset() : resets the environment to a random initial state\n",
    "- act(action) : perform `action` in the environment and return a tuple `(new_state, reward, done, infos)` (`info` is empty in the environments we'll use)\n",
    "- set_render(bool) : set the graphic rendering to True or False\n",
    "- close() : close the environment and the graphic window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's build a convolutional neural network architecture made of two convolution layers followed by a fully connected and a LSTM network with finally two fully connected layers to estimate the policy (ACTION_SIZE outputs) and the state value (1 output).\n",
    "\n",
    "**NB :** every variable with capital letters (like ACTION_SIZE) is defined in **settings.py** and we import it in the beginning of each file of the code, which I didn't report here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_state():\n",
    "    \"\"\"Return a tensor representing an empty LSTM state for a network with 256 cells\"\"\"\n",
    "    return tf.contrib.rnn.LSTMStateTuple(np.zeros([1, 256]),\n",
    "                                         np.zeros([1, 256]))\n",
    "\n",
    "def build_layers(device, thread_index):\n",
    "\n",
    "    scope_name = \"net_\" + str(thread_index)\n",
    "    with tf.device(device), tf.variable_scope(scope_name) as scope:\n",
    "        state = tf.placeholder(\"float\", [None, 84, 84, 4])\n",
    "\n",
    "        conv1 = tf.layers.conv2d(state, 64, [8, 8],\n",
    "                                 strides=4, padding=\"valid\",\n",
    "                                 activation=tf.nn.relu)\n",
    "        conv2 = tf.layers.conv2d(conv1, 128, [4, 4],s\n",
    "                                 strides=2, padding=\"valid\",\n",
    "                                 activation=tf.nn.relu)\n",
    "\n",
    "        conv2_flat = tf.contrib.layers.flatten(h_conv2)\n",
    "        hidden = tf.layers.dense(conv2_flat, 256, activation=tf.nn.relu)\n",
    "        hidden_reshaped = tf.reshape(h_fc1, [1, -1, 256])\n",
    "\n",
    "        # Placeholder for LSTM unrolling time step size.\n",
    "        step_size = tf.placeholder(tf.float32, [1])\n",
    "\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(256, state_is_tuple=True)\n",
    "        \n",
    "        initial_lstm_state_c = tf.placeholder(tf.float32, [1, 256])\n",
    "        initial_lstm_state_h = tf.placeholder(tf.float32, [1, 256])\n",
    "        initial_lstm_state = tf.contrib.rnn.LSTMStateTuple(initial_lstm_state_c,\n",
    "                                                           initial_lstm_state_h)\n",
    "\n",
    "        lstm_outputs, lstm_state = tf.nn.dynamic_rnn(lstm,\n",
    "                                                     hidden_reshaped,\n",
    "                                                     initial_state=initial_lstm_state,\n",
    "                                                     sequence_length=step_size,\n",
    "                                                     time_major=False,\n",
    "                                                     scope=scope)\n",
    "\n",
    "        lstm_outputs = tf.reshape(lstm_outputs, [-1, 256])\n",
    "\n",
    "        # Policy estimation\n",
    "        policy = tf.layers.dense(lstm_outputs, ACTION_SIZE, activation=tf.nn.softmax)\n",
    "\n",
    "        # Value estimation\n",
    "        v_ = tf.layers.dense(lstm_outputs, 1, activation=None)\n",
    "        value = tf.reshape(v_, [-1])\n",
    "        \n",
    "        return policy, value, initial_lstm_state, lstm_state\n",
    "    \n",
    "def get_vars(network_scope):\n",
    "    return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=network_scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the tensorflow operations to compute the policy and value loss. To encourage exploration, we regularize the policy loss with an entropy term that favor the network to explore as long as it lacks information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss():\n",
    "\n",
    "    action = tf.placeholder(\"float\", [None, ACTION_SIZE])\n",
    "    reward = tf.placeholder(\"float\", [None])\n",
    "    td_error = tf.placeholder(\"float\", [None])\n",
    "\n",
    "    log_pi = tf.log(tf.clip_by_value(policy, 1e-20, 1.0))  # We clip to avoid log(0) error\n",
    "\n",
    "    entropy = -tf.reduce_sum(policy * log_pi, reduction_indices=1)\n",
    "\n",
    "    policy_loss = - tf.reduce_sum(tf.reduce_sum(\n",
    "        tf.multiply(log_pi, action), reduction_indices=1) *\n",
    "        td_error + entropy * ENTROPY_REG)\n",
    "\n",
    "    value_loss = 0.5 * tf.nn.l2_loss(reward - value)\n",
    "\n",
    "    total_loss = policy_loss + value_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function that copies the weights from a network in a scope and pastes them to another network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_network(dst_network, src_network, name=None):\n",
    "    src_vars = get_vars(src_network)\n",
    "    dst_vars = get_vars(dst_network)\n",
    "\n",
    "    copy_ops = []\n",
    "    with tf.name_scope(name, \"Network\", []) as name:\n",
    "        for(src_var, dst_var) in zip(src_vars, dst_vars):\n",
    "            copy_ops.append(tf.assign(dst_var, src_var))\n",
    "\n",
    "        return tf.group(*copy_ops, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap all these functions in a class `Network` and we now can define agents that will run on different threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 thread_index,\n",
    "                 global_network,\n",
    "                 initial_learning_rate,\n",
    "                 learning_rate_input,\n",
    "                 grad_applier):\n",
    "\n",
    "        self.thread_index = thread_index\n",
    "        self.learning_rate_input = learning_rate_input\n",
    "\n",
    "        self.local_network = Network(thread_index)\n",
    "        self.local_network.build_loss()\n",
    "\n",
    "        local_var_refs = [v._ref() for v in self.local_network.get_vars()]\n",
    "\n",
    "        self.gradients = tf.gradients(self.local_network.total_loss,\n",
    "                                      local_var_refs)\n",
    "\n",
    "        # grad_applier is the shared RMS Prop defined in a separated file \n",
    "        self.apply_gradients = grad_applier.apply_gradients(\n",
    "            global_network.get_vars(),\n",
    "            self.gradients)\n",
    "\n",
    "        self.update_network = self.local_network.copy_network(global_network)\n",
    "\n",
    "        # We create a new environment from the wrapper class of Environment.py\n",
    "        self.env = Environment(thread_index == 1)\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.episode_reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must train our agent, so we define a `process` method that will make the agent interacts with its environment for at most UPDATE_FREQ steps, then compute the gradient from this direct minibatch of experience and apply it to the global network. At each call of `process`, if the episode was not finished, the agent resumes the last episode and interacts for UPDATE_FREQ more steps.<br/>\n",
    "We also progressively anneal the learning rate to be finer on the learning and not to take too important steps during the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(self, sess, total_steps):\n",
    "\n",
    "    buffer = []\n",
    "    done = False\n",
    "    episode_step = 0\n",
    "    start_lstm_state = self.local_network.lstm_state_out\n",
    "\n",
    "    # Copy weights from global to local network\n",
    "    sess.run(self.update_network)\n",
    "\n",
    "    for i in range(UPDATE_FREQ):\n",
    "\n",
    "        pi, value = self.local_network.run_policy_and_value(sess, self.state)\n",
    "        a = np.random.choice(ACTION_SIZE, p=pi)\n",
    "        s_, r, terminal, _ = self.env.process(a)\n",
    "\n",
    "        self.episode_reward += r\n",
    "        buffer.append((self.state, a, r, value))\n",
    "\n",
    "        episode_step += 1\n",
    "        self.state = s_\n",
    "\n",
    "        if terminal:\n",
    "            done = True\n",
    "            self.episode_reward = 0\n",
    "            self.env.reset()\n",
    "            self.local_network.reset_state()\n",
    "            break\n",
    "\n",
    "    batch_s = deque()\n",
    "    batch_a = deque()\n",
    "    batch_td = deque()\n",
    "    batch_R = deque()\n",
    "\n",
    "    # Bootstrapping the discounted reward\n",
    "    R = 0.0\n",
    "    if not done:\n",
    "        R = self.local_network.run_value(sess, self.state)\n",
    "\n",
    "    # Prepare the memory buffers and compute the N-Step return\n",
    "    for i in range(len(buffer) - 1, -1, -1):\n",
    "        si, ai, ri, Vi = buffer[i]\n",
    "        R = ri + GAMMA * R\n",
    "        td = R - Vi\n",
    "        a = np.zeros([ACTION_SIZE])\n",
    "        a[ai] = 1\n",
    "\n",
    "        batch_s.appendleft(si)\n",
    "        batch_a.appendleft(a)\n",
    "        batch_td.appendleft(td)\n",
    "        batch_R.appendleft(R)\n",
    "\n",
    "    cur_learning_rate = self.initial_learning_rate * (MAX_TIME_STEP - total_steps) / MAX_TIME_STEP\n",
    "\n",
    "    feed_dict = {self.local_network.state: batch_s,\n",
    "                 self.local_network.action: batch_a,\n",
    "                 self.local_network.td_error: batch_td,\n",
    "                 self.local_network.reward: batch_R,\n",
    "                 self.local_network.initial_lstm_state: start_lstm_state,\n",
    "                 self.local_network.step_size: [len(batch_a)],\n",
    "                 self.learning_rate_input: cur_learning_rate}\n",
    "    sess.run(self.apply_gradients, feed_dict=feed_dict)\n",
    "    \n",
    "    return episode_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in **main.py**, we create the global network and several parallel agents and run them asynchronously. Each agent has its own initial learning rate log-uniformly distributed between generally 1e-4 and 1e-2 which reduce the hyperparameter dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a log-uniform distribution for the learning rates initializer\n",
    "def log_uniform(lo, hi, rate):\n",
    "    log_lo = math.log(lo)\n",
    "    log_hi = math.log(hi)\n",
    "    v = log_lo * (1 - rate) + log_hi * rate\n",
    "    return math.exp(v)\n",
    "\n",
    "def work(worker_index):\n",
    "\n",
    "    worker = workers[worker_index]\n",
    "    while not stop_requested and total_steps <= MAX_TIME_STEP:\n",
    "        steps = worker.process(sess, total_steps)\n",
    "        total_steps += steps\n",
    "\n",
    "    worker.close()\n",
    "\n",
    "# What to do when `Ctrl+C` is sent\n",
    "def signal_handler(signal, frame):\n",
    "    global stop_requested\n",
    "    stop_requested = True\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    initial_learning_rates = log_uniform(1e-4, 1e-2, 0.4)\n",
    "    stop_requested = False\n",
    "\n",
    "    # Creation of the global network and the shared RMS Prop Optimizer\n",
    "    global_network = Network(0)\n",
    "\n",
    "    learning_rate_input = tf.placeholder(\"float\")\n",
    "\n",
    "    grad_applier = RMSPropApplier(learning_rate=learning_rate_input,\n",
    "                                  decay=RMSPROP_DECAY,\n",
    "                                  momentum=0.0,\n",
    "                                  epsilon=RMSPROP_EPSILON,\n",
    "                                  clip_norm=MAX_GRADIENT_NORM,\n",
    "                                  device=device)\n",
    "\n",
    "    # Create and initialize the workers\n",
    "    workers = []\n",
    "    for i in range(NB_THREADS):\n",
    "        worker = Agent(i + 1, global_network,\n",
    "                       initial_learning_rates,\n",
    "                       learning_rate_input,\n",
    "                       grad_applier)\n",
    "\n",
    "        workers.append(worker)\n",
    "\n",
    "    # prepare session\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create one thread per worker\n",
    "    train_threads = []\n",
    "    for i in range(NB_THREADS):\n",
    "        train_threads.append(threading.Thread(target=work, args=(i,)))\n",
    "\n",
    "    # Intercept CTRL+C signal\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "    # Start the workers' thread\n",
    "    for t in train_threads:\n",
    "        t.start()\n",
    "\n",
    "    # Wait for interruption\n",
    "    print('Press Ctrl+C to stop')\n",
    "    signal.pause()\n",
    "\n",
    "    for t in train_threads:\n",
    "        t.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Because of its parallelism, A3C is a great algorithm to solve Atari Games in the Arcade Learning Environment. The most simple game is probably Pong but even with that game it is still quite long to run the algorithm until the solving. For this game, I ran it during one night with 12 parallel threads.<br/>\n",
    "The left figure shows the evolution of the episode reward of an agent during the training : in the beginning, the agent has a score around -20 which represents 21/1 scores (the winner in Pong is the first one to reach 21 points) whereas in the end it is able to destroy its opponent and win 21 to 0.<br/>\n",
    "*The interesting curve is the red one, the others are the residues of other learning curves*\n",
    "\n",
    "\n",
    "![](./Images/A3C_results.png) | ![](./Images/Pong.gif)\n",
    ":-:                            | :-:\n",
    "Raw and mean episode rewards   | The agent in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "We can visualize on the gif that, even if the learned policy is quite efficient to beat its opponent (it found the weak points : the bouncy shots), the pad is shaky just after it hit the ball. It is because the IA actions won't influence the game at all while the ball is not coming back to the right side, so the algorithm couldn't learn any behavior to have during these phases.<br/>\n",
    "One way to avoid this shaky behavior would be the implementation of FiGAR [[Sharma et al.](https://arxiv.org/pdf/1702.06054.pdf)] : during the training, the agent learns the actions to perform but also how many times to repeat these actions. So in this situation, it could learn to wait without moving during nearly 1 second just after having hit the ball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept\n",
    "Deterministic Policy Gradient method has been introduced in 2014 [[Silver et al](http://proceedings.mlr.press/v32/silver14.pdf)] and extended as Deep DPG in 2016 [[Lillicrap et al.](https://arxiv.org/pdf/1509.02971.pdf)] to deal with continous control problems. Unlike Atari environments and gaming in general, domains such as robotic need to act continuously on motors for instance, and discretization of the action space is not possible due to the curse of dimensionality.\n",
    "\n",
    "The idea behind DDPG is quite intuitive : traditionnaly, the method to predict an action over a discrete space is to apply a softmax function over the output of the neural network to get a distribution of probability on the action space.\n",
    "Here, to build an agent that directly predicts the action to take, we remove the softmax activation function on the output layer and instead apply a sigmoid (or a tanh) that scales the value into [0, 1] (or [-1, 1]). A linear transformation can then rescale this output to the desired interval.\n",
    "\n",
    "In this architecture, the **critic** is trained as usual with the Bellman Equation, whereas the **actor** is updated by applying the policy gradient presented in the original DPG paper. As in DQN, a target network is also used to evaluate the Q-Value of chosen actions (see [this article section 3](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df) by Arthur Juliani for more details on the use of target networks). However, to update the target network, we continually and slowly affect the main network's weights to the target network : $\\theta_{target} \\leftarrow \\tau \\theta_{main} + (1 - \\tau) \\theta_{target}$.\n",
    "\n",
    "To force exploration, instead of using the usual $\\epsilon$-greedy policy, we add a time-correlated noise to action scaled by a decreasing factor : $noise_t = \\theta * (\\mu - noise_{t-1}) + \\sigma * normal(0, 1)$ with $normal$ the normal distribution : $$a_t += noise\\_scale_t * noise_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "*This code can be found in ./BlogFiles/Code/DDPG/*\n",
    "\n",
    "The general outline of the code architecture is quite similar to the A3C code, the main difference is that there is now an actor network and a critic network.\n",
    "Also, we need an experience memory buffer as in DQN which is implemented in **ExperienceBuffer.py**.\n",
    "\n",
    "First, let's see how to implement the actor network. <br/>\n",
    "We define a function to generate a neural network with 3 hidden fully connected layer of each 8 neurons which takes in parameters a placeholder for state inputs and two booleans `trainable` (the main network will be, the target one won't) and `reuse` to give the possibility to use the same network with a different input placeholder.<br/>\n",
    "This network outputs ACTION_SIZE values in [0, 1] (because of the sigmoid function) that we scale between LOW_BOUND and HIGH_BOUND line 17 with a linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actor definition :\n",
    "def generate_actor_network(states, trainable, reuse):\n",
    "    hidden = tf.layers.dense(states, 8,\n",
    "                             trainable=trainable, reuse=reuse,\n",
    "                             activation=tf.nn.relu, name='dense')\n",
    "    hidden_2 = tf.layers.dense(hidden, 8,\n",
    "                               trainable=trainable, reuse=reuse,\n",
    "                               activation=tf.nn.relu, name='dense_1')\n",
    "    hidden_3 = tf.layers.dense(hidden_2, 8,\n",
    "                               trainable=trainable, reuse=reuse,\n",
    "                               activation=tf.nn.relu, name='dense_2')\n",
    "    actions_unscaled = tf.layers.dense(hidden_3, ACTION_SIZE\n",
    "                                       trainable=trainable, reuse=reuse,\n",
    "                                       name='dense_3')\n",
    "    # bound the actions to the valid range\n",
    "    valid_range = HIGH_BOUND - LOW_BOUND\n",
    "    actions = LOW_BOUND + tf.nn.sigmoid(actions_unscaled) * valid_range\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in the discrete case, we can't predict the Q-Value of every possible action so our Q Network will also take an action as input to compute $Q(s, a)$. The rest is very similar to the actor definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Critic definition :\n",
    "def generate_critic_network(states, actions, trainable, reuse):\n",
    "    state_action = tf.concat([states, actions], axis=1)\n",
    "    hidden = tf.layers.dense(state_action, 8,\n",
    "                             trainable=trainable, reuse=reuse,\n",
    "                             activation=tf.nn.relu, name='dense')\n",
    "    hidden_2 = tf.layers.dense(hidden, 8,\n",
    "                               trainable=trainable, reuse=reuse,\n",
    "                               activation=tf.nn.relu, name='dense_1')\n",
    "    hidden_3 = tf.layers.dense(hidden_2, 8,\n",
    "                               trainable=trainable, reuse=reuse,\n",
    "                               activation=tf.nn.relu, name='dense_2')\n",
    "    q_values = tf.layers.dense(hidden_3, 1,\n",
    "                               trainable=trainable, reuse=reuse,\n",
    "                               name='dense_3')\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define our different placeholders and generate our actor networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Experience placeholders\n",
    "state_ph = tf.placeholder(dtype=tf.float32, shape=[None, STATE_SIZE])\n",
    "action_ph = tf.placeholder(dtype=tf.float32, shape=[None, ACTION_SIZE])\n",
    "reward_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "next_state_ph = tf.placeholder(dtype=tf.float32, shape=[None, STATE_SIZE])\n",
    "is_not_terminal_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "\n",
    "# Main actor network\n",
    "with tf.variable_scope('actor'):\n",
    "    actions = generate_actor_network(state_ph, trainable=True, reuse=False)\n",
    "\n",
    "# Target actor network\n",
    "with tf.variable_scope('slow_target_actor', reuse=False):\n",
    "    # tf.stop_gradient is an insurance that the network won't be affected by the gradient descent\n",
    "    target_next_actions = tf.stop_gradient(\n",
    "        generate_actor_network(next_state_ph, trainable=False, reuse=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation of the critic networks is trickier : for the main network, we call the `generate` function twice, once to compute the Q-Value of the actions that we actually took $Q(s, a)$, and once to compute the Q-Value of the actions that our actor network would have chosen $Q(s, Actor(s))$ (used to train the actor network).<br/>\n",
    "For the temporal difference, like in SARSA, the target network predicts the Q-Value of the next state we were in and the action that the actor would have taken in that state $Q_{target}(s', a')$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main critic network\n",
    "with tf.variable_scope('critic') as scope:\n",
    "    \n",
    "    # Critic applied to state_ph and action_ph (to train critic as usual)\n",
    "    q_values_of_given_actions = generate_critic_network(\n",
    "        state_ph, action_ph, trainable=True, reuse=False)\n",
    "    \n",
    "    # Critic applied to state_ph and the current policy's outputted\n",
    "    # actions for state_ph (to train actor)\n",
    "    q_values_of_suggested_actions = generate_critic_network(\n",
    "        state_ph, actions, trainable=True, reuse=True)\n",
    "\n",
    "# Target critic network\n",
    "with tf.variable_scope('slow_target_critic', reuse=False):\n",
    "    # Target critic applied to target actor's outputted\n",
    "    # actions for next_state_ph (to train critic)\n",
    "    q_values_next = tf.stop_gradient(\n",
    "        generate_critic_network(next_state_ph,\n",
    "                                target_next_actions,\n",
    "                                trainable=False, reuse=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute that temporal difference $$TD = r_t + \\gamma Q_{target}(s', a') - Q(s, a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One step TD targets y_i for (s,a) from experience replay\n",
    "# = r_i + GAMMA * Q_target(s',Actor(s')) if s' is not terminal\n",
    "# = r_i if s' terminal\n",
    "targets = tf.expand_dims(reward_ph, 1) + \\\n",
    "          tf.expand_dims(is_not_terminal_ph, 1) * GAMMA * q_values_next\n",
    "\n",
    "td_errors = targets - q_values_of_given_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we compute the loss, apply the gradient descent on our networks and define the operations to update the target networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Isolate vars for each network\n",
    "actor_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
    "target_actor_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_actor')\n",
    "critic_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic')\n",
    "target_critic_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_critic')\n",
    "\n",
    "\n",
    "# Critic loss and optimization\n",
    "critic_loss = tf.reduce_mean(tf.square(td_errors))\n",
    "\n",
    "critic_trainer = tf.train.AdamOptimizer(CRITIC_LEARNING_RATE)\n",
    "critic_train_op = critic_trainer.minimize(critic_loss)\n",
    "\n",
    "# Actor loss and optimization\n",
    "actor_loss = -1 * tf.reduce_mean(q_values_of_suggested_actions)\n",
    "\n",
    "actor_trainer = tf.train.AdamOptimizer(ACTOR_LEARNING_RATE)\n",
    "actor_train_op = actor_trainer.minimize(actor_loss, var_list=self.actor_vars)\n",
    "\n",
    "# Update values for slowly-changing targets towards current actor and critic\n",
    "tau = UPDATE_TARGET_RATE\n",
    "update_target_ops = []\n",
    "for i, target_actor_var in enumerate(target_actor_vars):\n",
    "    update_target_actor_op = target_actor_var.assign(tau * actor_vars[i] + (1 - tau) * target_actor_var)\n",
    "    update_target_ops.append(update_target_actor_op)\n",
    "\n",
    "for i, target_critic_var in enumerate(target_critic_vars):\n",
    "    update_target_critic_op = target_critic_var.assign(tau * critic_vars[i] + (1 - tau) * target_critic_var)\n",
    "    update_target_ops.append(update_target_critic_op)\n",
    "\n",
    "self.update_targets_op = tf.group(*update_target_ops, name='update_targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experience buffer is just a list to memorize experiences and sample them (we use the data structure *deque* which acts as a list with the extra ability to limit its size not to store too many experiences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "BUFFER_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class ExperienceBuffer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.buffer, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the agent with its network and its environment (like in A3C), and a run method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(self):\n",
    "\n",
    "    for ep in range(1, TRAINING_STEPS+1):\n",
    "\n",
    "        episode_reward = 0\n",
    "        episode_step = 0\n",
    "        done = False\n",
    "\n",
    "        # Initialize exploration noise process\n",
    "        noise_process = np.zeros(ACTION_SIZE)\n",
    "        noise_scale = (NOISE_SCALE_INIT * NOISE_DECAY**ep) * (HIGH_BOUND - LOW_BOUND)\n",
    "\n",
    "        # Initial state\n",
    "        s = self.env.reset()\n",
    "\n",
    "        while episode_step < MAX_EPISODE_STEPS and not done:\n",
    "\n",
    "            # choose action based on deterministic policy\n",
    "            a, = self.sess.run(self.network.actions,\n",
    "                               feed_dict={self.network.state_ph: s[None]})\n",
    "\n",
    "            # add temporally-correlated exploration noise to action\n",
    "            # (using an Ornstein-Uhlenbeck process)\n",
    "            noise_process = EXPLO_THETA * \\\n",
    "                (EXPLO_MU - noise_process) + \\\n",
    "                EXPLO_SIGMA * np.random.randn(ACTION_SIZE)\n",
    "\n",
    "            a += noise_scale * noise_process\n",
    "\n",
    "            s_, r, done, info = self.env.act(a)\n",
    "            episode_reward += r\n",
    "\n",
    "            self.buffer.add((s, a, r, s_, 0.0 if done else 1.0))\n",
    "\n",
    "            # update network weights to fit a minibatch of experience\n",
    "            if self.total_steps % TRAINING_FREQ == 0 and len(self.buffer) >= BATCH_SIZE:\n",
    "\n",
    "                minibatch = self.buffer.sample()\n",
    "                feed_dict = {self.network.state_ph: np.asarray([elem[0] for elem in minibatch]),\n",
    "                             self.network.action_ph: np.asarray([elem[1] for elem in minibatch]),\n",
    "                             self.network.reward_ph: np.asarray([elem[2] for elem in minibatch]),\n",
    "                             self.network.next_state_ph: np.asarray([elem[3] for elem in minibatch]),\n",
    "                             self.network.is_not_terminal_ph: np.asarray([elem[4] for elem in minibatch])}\n",
    "\n",
    "                _, _ = self.sess.run([self.network.critic_train_op, self.network.actor_train_op],\n",
    "                                     feed_dict=feed_dict)\n",
    "\n",
    "                # update target networks\n",
    "                _ = self.sess.run(self.network.update_slow_targets_op)\n",
    "\n",
    "            s = s_\n",
    "            episode_step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB :** In the real code, everything is wrapped up in classes and there are a few more files but it's mostly to set up the interface and get a cleaner code, the core of the algorithm has been presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "A classic environment to test DDPG is Pendulum on [Gym](http://gym.openai.com/envs/Pendulum-v0/).\n",
    "The goal here is to apply a torque on the pendulum to swing it up so it stays upright.\n",
    "Execute the cell below to see an exemple of interactions with the environment : we first apply a random torque every time step, and then just apply a counterclockwise torque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "    env.step(env.action_space.sample() if i < 100 else [2])\n",
    "    env.render()\n",
    "env.render(close=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that environment, the reward is given at each time step according to the torque applied to the pendulum, so depending on the starting point it may take a few steps to reach the top, that's why the maximum score reachable is in average around -100.\n",
    "\n",
    "We can easily test our program on that simple environment (it usually takes a few hundred episodes to stabilize around -100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "os.chdir(JUPYTER_PATH + \"/Code/DDPG/\")\n",
    "import tensorflow as tf\n",
    "import Agent\n",
    "importlib.reload(Agent)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    agent = Agent.Agent(sess)\n",
    "    try:\n",
    "        agent.run()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    agent.play(1)\n",
    "    agent.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this algorithm converges pretty quickly in that simple environment and we can visualize the final policy which is quite effective, even if on the end it sometimes fails to keep the pendulum exactly on the vertical. This could be because the tiny negative reward it gets on the end by keeping a small effort on the pendulum is \"hidden\" by the large rewards from the swing at the beginning.\n",
    "\n",
    "\n",
    "![](./Images/DDPG_results.png) | ![](./Images/Pendulum.gif)\n",
    ":-:                            | :-:\n",
    "Raw and mean episode rewards   | The agent in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting continuous environment is the [Bipedal Walker](http://gym.openai.com/envs/BipedalWalker-v2/) from gym where the goal is to apply torques on each part of the legs of a simple walker to make it go as far as possible. \n",
    "From Gym documentation :\n",
    "> Reward is given for moving forward, total 300+ points up to the far end. If the robot falls, it gets -100. Applying motor torque costs a small amount of points, more optimal agent will get better score. State consists of hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, legs contact with ground, and 10 lidar rangefinder measurements. There's no coordinates in the state vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"BipedalWalker-v2\")\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "    env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "env.render(close=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This environment is more challenging because of the size of the observation space : at each time step, the agent receives 24 different values (instead of 3 for the Pendulum).\n",
    "\n",
    "We can run the same algorithm to solve this environment, except it will take more time to achieve satisfying results. I ran it for about 6 hours \n",
    "\n",
    "\n",
    "\n",
    "Results after 4h                  | Results after 6h\n",
    ":-:                               | :-:\n",
    "![](./Images/BipedalWalker_1.gif) | ![](./Images/BipedalWalker_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* A possible amelioration is to upgrade the experience buffer to a prioritized one [[Schaul et al.](https://arxiv.org/pdf/1511.05952.pdf)] (see [Rainbow DQN section](#Rainbow-DQN) for more details on that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainbow DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept\n",
    "Since the apparition of DQN in 2013, many improvements have been developped to enhance the basic algorithm such as Double DQN or C51, but no one had tried to combine all these ameliorations until October 2017 with the publication of the Rainbow algorithm [[Hessel et al.](https://arxiv.org/pdf/1710.02298.pdf)].\n",
    "\n",
    "Five main improvements were used in that algorithm :\n",
    "- Double DQN [[Van Hasselt et al.](https://arxiv.org/pdf/1509.06461.pdf)]\n",
    "- Dueling DQN [[Wang et al.](https://arxiv.org/pdf/1511.06581.pdf)]\n",
    "- Prioritized Experience Replay [[Schaul et al.](https://arxiv.org/pdf/1511.05952.pdf)]\n",
    "- Distributional DQN [[Bellemare et al.](https://arxiv.org/abs/1707.06887)]\n",
    "- Noisy DQN [[Fortunato et al.](https://arxiv.org/pdf/1706.10295.pdf)]\n",
    "\n",
    "![](./Images/Rainbow.jpg)\n",
    "\n",
    "I won't describe all these enhancements in details since a better description is given on the [related blog post](https://rlsupaero.wordpress.com/2017/11/08/deep-rl-key-papers/) (along other approaches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "*This code can be found in ./BlogFiles/Code/Rainbow/*\n",
    "\n",
    "\n",
    "The benefit of this algorithm is that it doesn't require a whole new architecture, we can just start from a classical DQN implementation and add features on top of it. That's what I did with the double and the dueling DQN, and with PER but I didn't have time to add Distributional and Noisy DQN yet.\n",
    "\n",
    "The implementation is quite simple and the general outline of the code is the same as the other codes (A3C and DDPG). As usual, let's analyze the code for the network architecture and the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, let's build a dueling convolutional network. The network is composed of three convolutional layers followed by two streams of fully connected layers that output the advantage of each action (ACTION_SIZE outputs) and the value of the input state (1 output).\n",
    "\n",
    "![](./Images/dueling_dqn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    inputs = tf.placeholder(tf.float32, [None, *STATE_SIZE],\n",
    "                                 name='Input_state')\n",
    "\n",
    "    with tf.variable_scope('Convolutional_Layers'):\n",
    "        conv1 = tf.layers.conv2d(inputs=inputs, filters=32,\n",
    "                                 kernel_size=[8, 8], strides=[4, 4],\n",
    "                                 padding='VALID', activation=tf.nn.relu)\n",
    "        conv2 = tf.layers.conv2d(inputs=conv1, filters=64,\n",
    "                                 kernel_size=[4, 4], strides=[2, 2],\n",
    "                                 padding='VALID', activation=tf.nn.relu)\n",
    "        conv3 = tf.layers.conv2d(inputs=conv2, filters=64,\n",
    "                                 kernel_size=[3, 3], strides=[1, 1],\n",
    "                                 padding='VALID', activation=tf.nn.relu)\n",
    "\n",
    "        # Flatten the output\n",
    "        hidden = tf.contrib.layers.flatten(conv3)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def dueling(hidden):\n",
    "\n",
    "    advantage_stream = tf.dense(hidden, 32, activation=tf.nn.relu)\n",
    "    value_stream = tf.dense(hidden, 32, activation=tf.nn.relu)\n",
    "\n",
    "    advantage = tf.dense(advantage_stream, ACTION_SIZE, activation=None)\n",
    "    value = tf.dense(value_stream, 1, activation=None)\n",
    "\n",
    "    return value, advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the network which combines these two outputs to get the Q-Value and define the operations to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Network\"):\n",
    "\n",
    "    # Define the model as above (wrapped in a class)\n",
    "    model = NetworkArchitecture(STATE_SIZE, ACTION_SIZE)\n",
    "\n",
    "    # Convolution network\n",
    "    inputs = model.build_model()\n",
    "\n",
    "    # Dueling DQN\n",
    "    value, advantage = model.dueling()\n",
    "\n",
    "    # Aggregation of value and advantage to get the Q-value\n",
    "    adv_mean = tf.reduce_mean(advantage, axis=1, keep_dims=True)\n",
    "    Qvalues = value + tf.subtract(advantage, adv_mean)\n",
    "\n",
    "    predict = tf.argmax(Qvalues, 1)\n",
    "\n",
    "    # Loss\n",
    "    Qtarget = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "    actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "    actions_onehot = tf.one_hot(actions, ACTION_SIZE, dtype=tf.float32)\n",
    "\n",
    "    Qaction = tf.reduce_sum(tf.multiply(Qvalues, actions_onehot), axis=1)\n",
    "\n",
    "    td_error = tf.square(Qtarget - Qaction)\n",
    "    loss = tf.reduce_mean(td_error)\n",
    "    \n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    train = trainer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define an agent with a main network, a target network and a Prioritized Replay Buffer.\n",
    "For the PER, I took the [OpenAI implementation](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py) from their `baselines` algo bank that implements the right data structure (a SumTree, *cf* the original PER paper for more information) and allows to add experiences and retrieve them efficiently and quickly.\n",
    "\n",
    "During the initialization of a PrioritizedReplayBuffer, we set the $\\alpha$ parameter which determines how much prioritization is used, and each time we sample a minibatch, we provide a $\\beta$ parameter called the \"importance-sampling parameter\" that control the bias introduced by PER. Concretely, we anneal $\\beta \\rightarrow 1$ to use a strong prioritization at the beginning of the learning (which introduce a strong bias) and progressively uniformize the picking probabilities.\n",
    "\n",
    "The main methods from the PrioritizedReplayBuffer structure are :\n",
    "- add(s, a, r, s', terminal)\n",
    "- sample(batch_size, beta)\n",
    "- update_priorities(indexes, weights)\n",
    "\n",
    "Here, we will use the proportional variant of PER that update the weights of experiences based on the TD-error : priority of transition $i$ : $p_i = |\\delta_i| + \\epsilon$ (to have $p_i > 0$).\n",
    "\n",
    "Additionnally, we implement N-step learning : instead of using a single reward as a target, we wait for multi-steps to get a better estimation of the reward which slightly modify the loss definition : $$Loss = \\left[\\sum_{k=0}^{n-1}{\\gamma^k r_{t+k}} + \\gamma^n \\max_a{Q(s_{t+n}, a)} - Q(s_t, a_t)\\right]^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, sess):\n",
    "        \n",
    "        [...]\n",
    "        self.env = Environment()\n",
    "\n",
    "        self.mainQNetwork = QNetwork(STATE_SIZE, ACTION_SIZE, 'main')\n",
    "        self.targetQNetwork = QNetwork(STATE_SIZE, ACTION_SIZE, 'target')\n",
    "        \n",
    "        self.buffer = PrioritizedReplayBuffer(BUFFER_SIZE, ALPHA)\n",
    "        [...]\n",
    "        \n",
    "    def run(self):\n",
    "\n",
    "        self.nb_ep = 1\n",
    "\n",
    "        while self.nb_ep < TRAINING_STEPS:\n",
    "\n",
    "            s = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_step = 0\n",
    "            done = False\n",
    "\n",
    "            memory = deque()\n",
    "\n",
    "            while episode_step < MAX_EPISODE_STEPS and not done:\n",
    "\n",
    "                # Epsilon-greedy policy\n",
    "                if random.random() < self.epsilon:\n",
    "                    a = random.randint(0, self.action_size - 1)\n",
    "                else:\n",
    "                    a = self.sess.run(self.mainQNetwork.predict,\n",
    "                                      feed_dict={self.mainQNetwork.inputs: [s]})\n",
    "                    a = a[0]\n",
    "\n",
    "                s_, r, done, info = self.env.act(a)\n",
    "                episode_reward += r\n",
    "\n",
    "                memory.append((s, a, r, s_, done))\n",
    "\n",
    "                # We have enough experience in the memory to compute the N-Step reward\n",
    "                if len(memory) > N_STEP_RETURN:\n",
    "                    s_mem, a_mem, r_mem, ss_mem, done_mem = memory.popleft()\n",
    "                    discount_R = r_mem\n",
    "                    for i, (si, ai, ri, s_i, di) in enumerate(memory):\n",
    "                        discount_R += ri * GAMMA ** (i+1)\n",
    "                    self.buffer.add(s_mem, a_mem, discount_R, s_, done)\n",
    "\n",
    "                if episode_step % TRAINING_FREQ == 0:\n",
    "\n",
    "                    train_batch = self.buffer.sample(BATCH_SIZE, self.beta)\n",
    "\n",
    "                    # Incr beta\n",
    "                    if self.beta <= BETA_STOP:\n",
    "                        self.beta += BETA_INCR\n",
    "\n",
    "                    feed_dict = {self.mainQNetwork.inputs: train_batch[3]}\n",
    "                    mainQaction = self.sess.run(self.mainQNetwork.predict,\n",
    "                                                feed_dict=feed_dict)\n",
    "\n",
    "                    feed_dict = {self.targetQNetwork.inputs: train_batch[3]}\n",
    "                    targetQvalues = self.sess.run(self.targetQNetwork.Qvalues,\n",
    "                                                  feed_dict=feed_dict)\n",
    "\n",
    "                    done_multiplier = (1 - train_batch[4])\n",
    "                    doubleQ = targetQvalues[range(BATCH_SIZE), mainQaction]\n",
    "                    targetQvalues = train_batch[2] + GAMMA * doubleQ * done_multiplier\n",
    "\n",
    "                    feed_dict = {self.mainQNetwork.inputs: train_batch[0],\n",
    "                                 self.mainQNetwork.Qtarget: targetQvalues,\n",
    "                                 self.mainQNetwork.actions: train_batch[1]}\n",
    "                    td_error, _ = self.sess.run([self.mainQNetwork.train,\n",
    "                                                  self.mainQNetwork.td_error],\n",
    "                                                 feed_dict=feed_dict)\n",
    "\n",
    "                    # Update the priorities of transitions (proportional variant)\n",
    "                    self.buffer.update_priorities(train_batch[6], td_error + 1e-6)\n",
    "                    update_target(self.update_target_ops, self.sess)\n",
    "\n",
    "                s = s_\n",
    "                episode_step += 1\n",
    "\n",
    "            self.nb_ep += 1\n",
    "\n",
    "            # Decay epsilon\n",
    "            if self.epsilon > EPSILON_STOP:\n",
    "                self.epsilon -= EPSILON_DECAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Another very classic example of environment is [CartPole](http://gym.openai.com/envs/CartPole-v0/) on gym :\n",
    "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "Let's visualize it by going left and right alternatively :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "for i in range(-15, 150):\n",
    "    env.step((i//30)%2)\n",
    "    env.render()\n",
    "env.render(close=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can try to solve this environment with our program<br/>\n",
    "\n",
    "(Warning ! The PrioritizedReplayBuffer object form OpenAI's baseline requires the library mpi4py !<br/>\n",
    "Just run `conda install mpi4py` or `pip install mpi4py` to install this library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import os\n",
    "try:os.chdir(JUPYTER_PATH + \"/Code/Rainbow/\")\n",
    "except FileNotFoundError:pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import Agent\n",
    "importlib.reload(Agent)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    agent = Agent.Agent(sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    agent.run()\n",
    "    agent.play(1)\n",
    "    agent.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning is quite fast (two minutes at most) due to the simplicity of the environment and the final policy is very effective, reaching the highest possible score almost every time. The few runs where it doesn't reach the highest score are due to the $\\epsilon$-greedy exploration policy which is still quite important before 1000 episodes.\n",
    "\n",
    "![](./Images/Rainbow_results.png) | ![](./Images/CartPole.gif)\n",
    ":-:                               | :-:\n",
    "Raw and mean episode rewards      | The agent in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to sove another environment [Lunar Lander](http://gym.openai.com/envs/LunarLander-v2/) where the goal is to land a lunar module on a pad by activating its motors.\n",
    "> Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "for i in range(120):\n",
    "    env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "env.render(close=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the Bipedal Walker environment, the computation is way longer than for CartPole (a few hours) but the end results are pretty satisfying\n",
    "\n",
    "![](./Images/LunarLander_results.png) | ![](./Images/LunarLander.gif)\n",
    ":-:                                   | :-:\n",
    "Mean episode rewards          | The agent in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We analyzed the implementation of three different kinds of RL algorithm : an asynchronous one, a continuous one and a more basic one. But reinforcement learning is evolving rapidly and these implementation are already starting to be outdated : for instance, the Rainbow algorithm is originally based on Distributional DQN, but the authors of this paper have published an even more efficient algorithm [[Dabney et al.](https://arxiv.org/pdf/1710.10044.pdf)] that could be added to Rainbow.<br/>\n",
    "Moreover, many smaller methods could be added that slightly improve performances, such as Generalized Advantage Learning or the UNREAL Architecture : these are not keystones but gathered it might make a noticeable difference.<br/>\n",
    "*see the [blog post](https://rlsupaero.wordpress.com/2017/11/08/deep-rl-key-papers/) for more details about that*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
