{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Reinforcement Learning *via* the Arcade Learning Environment and OpenAI's Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Background\n",
    "A quick recap on reinforcement learning and the notations we will adopt here : we address the problem of an **agent** learning to act in an **environment** in order to maximize a **reward** signal.\n",
    "\n",
    "\n",
    "At each time step, the environment provides an observation $s_t \\in S$, the agent responds by selecting an action $a_t \\in A$ and then the environment provides a reward $r_{t+1}$ and the next state $s_{t+1}$ with a discount factor $\\gamma \\in [0, 1]$.\n",
    "We assume that this system is a Markov Decision Process e.g. the state of the system at the time step $t+1$ only depends on the state and the action chose at the time step $t$.\n",
    "\n",
    "\n",
    "The goal of RL is to find a policy $\\pi$ (e.g. a probability distribution over actions for each state) that maximize the expected discounted return $G_t = \\sum_{k=0}^{+\\infty}{\\gamma^k R_{t+k+1}}$ over an episode in the environment.\n",
    "\n",
    "To do so, we learn an estimate of the **Q-Value function** $Q_\\pi(s_t, a_t) = E_\\pi[G_t|s_t, a_t]$ which is equal to the expected reward an agent will receive starting from a state $s_t$ and action $a_t$ and then acting under policy $\\pi$. Once this Q-Value function is known, an optimal policy is trivial : $\\pi(s_t) = max_{a \\in A}{Q(s_t, a)}$.\n",
    "\n",
    "Let's also define the **Value function** $V_\\pi(s_t) = E_\\pi[G_t|s_t]$ the expected discounted reward obtained following a policy $\\pi$ starting from a state $s_t$.\n",
    "\n",
    "\n",
    "*For more details, see the [wikipedia page on MDPs][wikiMDP].*\n",
    "\n",
    "\n",
    "\n",
    "### DQN Algorithm\n",
    "Theorically, some methods exist to compute the Q-Value function, such as tabular Q-Learning. The idea is to start from a random guess of the Q-Value of each pair (state, action) and iteratively apply [Bellman Equation][BellEq] until it converges toward the real value of the function *(see [this post by Arthur Juliani][JulianiPost] to get an example)*.\n",
    "\n",
    "\n",
    "But this method is only appliable on very few problems with tiny state and action spaces, and usual problems generally have nearly infinite large state space, and even if the idea is not bad, the tabular approach cannot work on them.\n",
    "\n",
    "\n",
    "Deep Reinforcement Learning is an efficient method to solve such problems. The idea is to approximate the Q-Value function with a deep neural network trained by gradient descent to then derive a well performing policy from it.\n",
    "It was successfully implemented for the first time in 2013 [[Mnih et al][DQN]] as DQN by combining the use of convolutional nets to efficiently processes the raw frames fed as input to the network and of a replay memory buffer not to learn only on immediate rewards.\n",
    "\n",
    "\n",
    "The optimization by gradient descent is realized wrt to the loss on a randomly chosen time step picked from the replay memory $$(R_t + \\gamma_{t+1} max_aQ_\\bar{\\theta}(s_{t+1}, a) - Q\\theta(s_t, a_t))^2$$ with $Q_\\bar{\\theta}$ the *online network* (the network used to select the action) and $Q_\\theta$ the *target network* (a copy of the network periodically updated to stabilize the learning).\n",
    "\n",
    "\n",
    "DQN was the first concrete example of successful reinforcement learning algorithm, and it opened the way to many researches and improvements since 2015. In this notebook, I will present three modified versions of DQN that has lead to improvements or that extended the application domain :\n",
    "* A3C : an asynchronous version that allows parallelism\n",
    "* DDPG : a continuous version\n",
    "* Rainbow DQN : an improved version of DQN, one of the most efficient algorithm of Deep RL today\n",
    "\n",
    "\n",
    "[wikiMDP]: https://en.wikipedia.org/wiki/Markov_decision_process\n",
    "[BellEq]: https://en.wikipedia.org/wiki/Bellman_equation#Example\n",
    "[JulianiPost]: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "[DQN]: https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Advantage Actor-Critic (A3C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept\n",
    "Presented in 2016 [[Mnih et al. again](https://arxiv.org/pdf/1602.01783.pdf)], A3C is quite different from regular DQN : it's an Actor-Critic. First, let's debunk the name of the algorithm to understand it better :\n",
    "+ **Asynchronous :** Asynchronous algorithm is a great family of algos designed to run on parallel architecture to speed the learning up. The main idea is instead of having only one agent interacting with an environment, an asynchronous algo creates a global network and many workers on parallel threads with each their own environment and their own neural network. Then, during an episode, each agent copies the weights of the global network, interacts with it's own environment and apply a gradient descent on the weights of the global network.\n",
    "\n",
    " This has two main advantages :\n",
    "  - the parallelism of today hardware allows to run tens or even hundreds of workers in the same time on different threads, and so to maximize the work done in a given time\n",
    "  - each worker being independent of the others, they collect a lot of various experience and assure a better exploration of the state space, removing the necessity to have a replay memory buffer and reducing the bias\n",
    "\n",
    "\n",
    "+ **Actor-Critic :** Instead of just estimating the Q-Value function and then inducing a policy by acting greedily with respect to the action Q-Values, in Actor-Critic algorithms, the network estimate both the Value $V$ function and a policy $\\pi$. The value estimator is called the **critic** and the policy estimator the **actor**.\n",
    "\n",
    "\n",
    "\n",
    "+ **Advantage :** We define advantage as the difference between the Q-Value and the Value of a given state and action $A(s, a) = Q(s, a) - V(s)$ : it represents how much better an action is than expected. During a gradient descent, the updates usually use discounted reward to tell the quality of a taken action, but one way to be more efficient is instead to use an advantage estimate to get how much better it is than on average.\n",
    "\n",
    " We can estimate this advantage quite easily because $V(s_t)$ is the output of our actor-critic network and $Q(s_t, a_t)$ can be estimated by the discounted reward $R_t = \\sum_{k=0}^{+\\infty}{\\gamma^k r_{t+k}}$ : $$A_{est}(s_t, a_t) = R_t - V(s_t)$$\n",
    " \n",
    "![](./Images/A3C.png \"Architecture of an A3C Network\")\n",
    "<center>*Image from [Arthur Juliani's blog](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2)*</center>\n",
    "\n",
    "Additionnaly, to help the network to understand time dependencies, we add a recurrent layer made of LSTM cells (see [Chris Olah's blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) for more details).\n",
    "\n",
    "### Implementation\n",
    "*This code can be found in ./BlogFiles/Code/A3C/*\n",
    "\n",
    "Let's now see an actual implementation of the A3C algorithm applied to a simple Atari 2600 game : Pong.\n",
    "First, the general outline of the code architecture :\n",
    "- **NetworkArchitecture.py** : defines a class that builds a neural network with convolution and LSTM layers\n",
    "- **Network.py** : defines a class that sets up a model (an instance of NetworkArchitecture) and Tensorflow operations to compute and apply gradients on the network weights\n",
    "- **Environment.py** : wrapper for [gym environments](https://github.com/openai/gym)\n",
    "- **Agent.py** : defines a worker that contains a local network and a local environment and that can interact within this environment to get experiences and train the global network\n",
    "- **main.py** : the main program that creates a global network and many workers, and run them on different threads\n",
    "- **parameters.py** : contains every important parameter that can be modified in the algorithm\n",
    "- **Displayer.py**, **Saver.py** : defines visualisation and saving tools\n",
    "\n",
    "\n",
    "Now, let's analyze more precisely the main lines of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's build a convolutional neural network architecture made of two convolution layers followed by a fully connected, and then a LSTM network :\n",
    "\n",
    "*From NetworkArchitecture.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "LSTM_CELLS = 256\n",
    "\n",
    "class NetworkArchitecture:\n",
    "\n",
    "    def __init__(self, state_size):\n",
    "        self.state_size = state_size\n",
    "\n",
    "    def build_conv(self):\n",
    "        \"\"\"Define a succesion of convolutional layers followed by a fully\n",
    "        connected layer and return the input placeholder\"\"\"\n",
    "\n",
    "        # Placeholder for the input states (e.g the raw pixel frames)\n",
    "        self.inputs = tf.placeholder(tf.float32, [None, *self.state_size],\n",
    "                                     name='Input_state')\n",
    "\n",
    "        with tf.variable_scope('Convolutional_Layers'):\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=[8, 8],\n",
    "                                          strides=[4, 4],\n",
    "                                          padding='valid',\n",
    "                                          activation=tf.nn.relu)\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[4, 4],\n",
    "                                          strides=[2, 2],\n",
    "                                          padding='valid',\n",
    "                                          activation=tf.nn.relu)\n",
    "\n",
    "        # Flatten the output\n",
    "        flat_conv2 = tf.layers.flatten(self.conv2)\n",
    "        self.hidden = tf.layers.dense(flat_conv2, 256, activation=tf.nn.elu)\n",
    "        return self.inputs\n",
    "\n",
    "    def build_lstm(self):\n",
    "\n",
    "        with tf.variable_scope('LSTM'):\n",
    "            # New LSTM Network with 256 cells\n",
    "            lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(LSTM_CELLS)\n",
    "            c_size = lstm_cell.state_size.c\n",
    "            h_size = lstm_cell.state_size.h\n",
    "\n",
    "            # Initial cell state\n",
    "            c_init = np.zeros((1, c_size), np.float32)\n",
    "            h_init = np.zeros((1, h_size), np.float32)\n",
    "            self.lstm_state_init = [c_init, h_init]\n",
    "\n",
    "            # Input state\n",
    "            c_in = tf.placeholder(tf.float32, [1, c_size])\n",
    "            h_in = tf.placeholder(tf.float32, [1, h_size])\n",
    "\n",
    "            # tf.nn.dynamic_rnn expects inputs of shape\n",
    "            # [batch_size, time, features], but the shape of hidden is\n",
    "            # [batch_size, features]. We want the batch_size dimension to\n",
    "            # be treated as the time dimension, so the input is redundantly\n",
    "            # expanded to [1, batch_size, features].\n",
    "            # The LSTM layer will assume it has 1 batch with a time\n",
    "            # dimension of length batch_size.\n",
    "            lstm_input = tf.expand_dims(self.hidden, [0])\n",
    "            step_size = tf.shape(self.inputs)[:1]\n",
    "            state_in = tf.nn.rnn_cell.LSTMStateTuple(c_in, h_in)\n",
    "\n",
    "            # Unroll the LSTM cells\n",
    "            lstm_output, lstm_state = tf.nn.dynamic_rnn(lstm_cell,\n",
    "                                                        lstm_input,\n",
    "                                                        step_size,\n",
    "                                                        state_in)\n",
    "            lstm_c, lstm_h = lstm_state\n",
    "            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "            self.output = tf.reshape(lstm_output, [-1, LSTM_CELLS])\n",
    "            return (c_in, h_in)\n",
    "\n",
    "    def return_output(self):\n",
    "        \"\"\"Return the state of the LSTM cells and their output\"\"\"\n",
    "        return self.state_out, self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the architecture of a neural network, we can build a complete network by appending two fully connected layers after the LSTM output to estimate the Value function and the policy and by defining the tensorflow operations for gradient descent.\n",
    "\n",
    "*From Network.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "MAX_GRADIENT_NORM = 40.0\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, state_size, action_size, scope):\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state_size = state_size\n",
    "            self.action_size = action_size\n",
    "\n",
    "            # Creation of the model\n",
    "            self.model = NetworkArchitecture(self.state_size)\n",
    "\n",
    "            # We create the convolution layers and get the input placeholder\n",
    "            self.inputs = self.model.build_conv()\n",
    "\n",
    "            # We create the LSTM network and get the input placeholder\n",
    "            self.state_in = self.model.build_lstm()\n",
    "\n",
    "            self.lstm_state_init = self.model.lstm_state_init\n",
    "            \n",
    "            # We get the output of the LSTM network and add two layers to estimate\n",
    "            # the value function and the policy\n",
    "            self.state_out, model_output = self.model.return_output()\n",
    "\n",
    "            self.policy = tf.layers.dense(model_output, action_size, activation=tf.nn.softmax)\n",
    "            self.value = tf.layers.dense(model_output, 1, activation=None)\n",
    "\n",
    "        # If it is not the global network\n",
    "        if scope != 'global':\n",
    "            self.actions = tf.placeholder(tf.int32, [None], 'Action')\n",
    "            self.actions_onehot = tf.one_hot(self.actions, self.action_size, dtype=tf.float32)\n",
    "            self.advantages = tf.placeholder(tf.float32, [None], 'Advantage')\n",
    "            self.discounted_reward = tf.placeholder(tf.float32, [None], 'Discounted_Reward')\n",
    "            \n",
    "            self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "            self.responsible_outputs = tf.clip_by_value(self.responsible_outputs, 1e-20, 1)  # Prevent from log(0)\n",
    "\n",
    "            # Estimate the policy loss and regularize it by adding uncertainty (= subtracting entropy)\n",
    "            self.policy_loss = -tf.reduce_sum(tf.multiply(\n",
    "                tf.log(self.responsible_outputs), self.advantages))\n",
    "            self.entropy = -tf.reduce_sum(self.policy * tf.log(self.policy))\n",
    "\n",
    "            # Estimate the value loss using the sum of squared errors.\n",
    "            self.value_loss = tf.reduce_sum(tf.square(self.advantages))\n",
    "\n",
    "            # Estimate the final loss.\n",
    "            self.loss = self.policy_loss + 0.5 * self.value_loss - 0.01 * self.entropy\n",
    "\n",
    "            # Fetch and clip the gradients of the local network.\n",
    "            local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "            gradients = tf.gradients(self.loss, local_vars)\n",
    "            clipped_gradients, self.grad_norm = tf.clip_by_global_norm(gradients, MAX_GRADIENT_NORM)\n",
    "\n",
    "            # Apply gradients to global network\n",
    "            global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "            optimizer = tf.train.AdamOptimizer(parameters.LEARNING_RATE)\n",
    "            grads_and_vars = zip(clipped_gradients, global_vars)\n",
    "            self.apply_grads = optimizer.apply_gradients(grads_and_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Because of its parallelism, A3C is a great algorithm to face the Atari Games resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept\n",
    "Deterministic Policy Gradient method has been introduced in 2014 [[Silver et al](http://proceedings.mlr.press/v32/silver14.pdf)] and extended as Deep DPG in 2016 [[Lillicrap et al.](https://arxiv.org/pdf/1509.02971.pdf)] to deal with continous control problems. Unlike Atari environments and gaming in general, domains such as robotic need to act continuously on motors for instance, and discretization of the action space is not possible due to the curse of dimensionality.\n",
    "\n",
    "The idea behind DDPG is quite intuitive : traditionnaly, the method to predict an action over a discrete space is to apply a softmax function over the output of the neural network to get a distribution of probability on the action space.\n",
    "Here, to build an agent that directly predicts the action to take, we remove the softmax activation function on the output layer and instead apply a sigmoid (or a tanh) that scales the value into [0, 1] (or [-1, 1]). A linear transformation can then rescale this output to the desired interval.\n",
    "\n",
    "In this architecture, the **critic** is trained as usual with the Bellman Equation, whereas the **actor** is updated by applying the policy gradient presented in the original DPG paper. As in DQN, a target network is also used to evaluate the Q-Value of chosen actions (see [this article section 3](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df) by Arthur Juliani for more details on the use of target networks). However, to update the target network, we continually and slowly affect the main network's weights to the target network : $\\theta_{target} \\leftarrow \\tau \\theta_{main} + (1 - \\tau) \\theta_{target}$.\n",
    "\n",
    "To force exploration, instead of using the usual $\\epsilon$-greedy policy, we add a time-correlated noise to action scaled by a decreasing factor : $noise_t = \\theta * (\\mu - noise_{t-1}) + \\sigma * normal(0, 1)$ with $normal$ the normal distribution : $$a_t += noise\\_scale_t * noise_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "*This code can be found in ./BlogFiles/Code/DDPG/*\n",
    "\n",
    "The general outline of the code architecture is quite similar to the A3C code, the main difference is that there is now an actor network and a critic network.\n",
    "Also, we need an experience memory buffer as in DQN which is implemented in **ExperienceBuffer.py**.\n",
    "\n",
    "First, let's see how to implement the actor network. <br/>\n",
    "We define a function to generate a neural network with 3 hidden fully connected layer of each 8 neurons which takes in parameters a placeholder for state inputs and two booleans `trainable` (the main network will be, the target one won't) and `reuse` to give the possibility to use the same network with a different input placeholder.<br/>\n",
    "This network outputs ACTION_SIZE values in [0, 1] (because of the sigmoid function) that we scale between LOW_BOUND and HIGH_BOUND line 17 with a linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actor definition :\n",
    "def generate_actor_network(states, trainable, reuse):\n",
    "    hidden = tf.layers.dense(states, 8,\n",
    "                             trainable=trainable, reuse=reuse,\n",
    "                             activation=tf.nn.relu, name='dense')\n",
    "    hidden_2 = tf.layers.dense(hidden, 8,\n",
    "                               trainable=trainable, reuse=reuse,\n",
    "                               activation=tf.nn.relu, name='dense_1')\n",
    "    hidden_3 = tf.layers.dense(hidden_2, 8,\n",
    "                               trainable=trainable, reuse=reuse,\n",
    "                               activation=tf.nn.relu, name='dense_2')\n",
    "    actions_unscaled = tf.layers.dense(hidden_3, ACTION_SIZE\n",
    "                                       trainable=trainable, reuse=reuse,\n",
    "                                       name='dense_3')\n",
    "    # bound the actions to the valid range\n",
    "    valid_range = HIGH_BOUND - LOW_BOUND\n",
    "    actions = LOW_BOUND + tf.nn.sigmoid(actions_unscaled) * valid_range\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in the discrete case, we can't predict the Q-Value of every possible action so our Q Network will also take an action as input to compute $Q(s, a)$. The rest is very similar to the actor definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Critic definition :\n",
    "def generate_critic_network(states, actions, trainable, reuse):\n",
    "    state_action = tf.concat([states, actions], axis=1)\n",
    "    hidden = tf.layers.dense(state_action, 8,\n",
    "                             trainable=trainable, reuse=reuse,\n",
    "                             activation=tf.nn.relu, name='dense')\n",
    "    hidden_2 = tf.layers.dense(hidden, 8,\n",
    "                               trainable=trainable, reuse=reuse,\n",
    "                               activation=tf.nn.relu, name='dense_1')\n",
    "    hidden_3 = tf.layers.dense(hidden_2, 8,\n",
    "                               trainable=trainable, reuse=reuse,\n",
    "                               activation=tf.nn.relu, name='dense_2')\n",
    "    q_values = tf.layers.dense(hidden_3, 1,\n",
    "                               trainable=trainable, reuse=reuse,\n",
    "                               name='dense_3')\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define our different placeholders and generate our actor networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Experience placeholders\n",
    "state_ph = tf.placeholder(dtype=tf.float32, shape=[None, STATE_SIZE])\n",
    "action_ph = tf.placeholder(dtype=tf.float32, shape=[None, ACTION_SIZE])\n",
    "reward_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "next_state_ph = tf.placeholder(dtype=tf.float32, shape=[None, STATE_SIZE])\n",
    "is_not_terminal_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "\n",
    "# Main actor network\n",
    "with tf.variable_scope('actor'):\n",
    "    actions = generate_actor_network(state_ph, trainable=True, reuse=False)\n",
    "\n",
    "# Target actor network\n",
    "with tf.variable_scope('slow_target_actor', reuse=False):\n",
    "    # tf.stop_gradient is an insurance that the network won't be affected by the gradient descent\n",
    "    target_next_actions = tf.stop_gradient(\n",
    "        generate_actor_network(next_state_ph, trainable=False, reuse=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation of the critic networks is trickier : for the main network, we call the `generate` function twice, once to compute the Q-Value of the actions that we actually took $Q(s, a)$, and once to compute the Q-Value of the actions that our actor network would have chosen $Q(s, Actor(s))$ (used to train the actor network).<br/>\n",
    "For the temporal difference, like in SARSA, the target network predicts the Q-Value of the next state we were in and the action that the actor would have taken in that state $Q_{target}(s', a')$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main critic network\n",
    "with tf.variable_scope('critic') as scope:\n",
    "    \n",
    "    # Critic applied to state_ph and action_ph (to train critic as usual)\n",
    "    q_values_of_given_actions = generate_critic_network(\n",
    "        state_ph, action_ph, trainable=True, reuse=False)\n",
    "    \n",
    "    # Critic applied to state_ph and the current policy's outputted\n",
    "    # actions for state_ph (to train actor)\n",
    "    q_values_of_suggested_actions = generate_critic_network(\n",
    "        state_ph, actions, trainable=True, reuse=True)\n",
    "\n",
    "# Target critic network\n",
    "with tf.variable_scope('slow_target_critic', reuse=False):\n",
    "    # Target critic applied to target actor's outputted\n",
    "    # actions for next_state_ph (to train critic)\n",
    "    q_values_next = tf.stop_gradient(\n",
    "        generate_critic_network(next_state_ph,\n",
    "                                target_next_actions,\n",
    "                                trainable=False, reuse=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute that temporal difference $$TD = r_t + \\gamma Q_{target}(s', a') - Q(s, a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One step TD targets y_i for (s,a) from experience replay\n",
    "# = r_i + GAMMA * Q_target(s',Actor(s')) if s' is not terminal\n",
    "# = r_i if s' terminal\n",
    "targets = tf.expand_dims(reward_ph, 1) + \\\n",
    "          tf.expand_dims(is_not_terminal_ph, 1) * GAMMA * q_values_next\n",
    "\n",
    "td_errors = targets - q_values_of_given_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we compute the loss, apply the gradient descent on our networks and define the operations to update the target networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Isolate vars for each network\n",
    "actor_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
    "target_actor_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_actor')\n",
    "critic_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic')\n",
    "target_critic_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_critic')\n",
    "\n",
    "\n",
    "# Critic loss and optimization\n",
    "critic_loss = tf.reduce_mean(tf.square(td_errors))\n",
    "\n",
    "critic_trainer = tf.train.AdamOptimizer(CRITIC_LEARNING_RATE)\n",
    "critic_train_op = critic_trainer.minimize(critic_loss)\n",
    "\n",
    "# Actor loss and optimization\n",
    "actor_loss = -1 * tf.reduce_mean(q_values_of_suggested_actions)\n",
    "\n",
    "actor_trainer = tf.train.AdamOptimizer(ACTOR_LEARNING_RATE)\n",
    "actor_train_op = actor_trainer.minimize(actor_loss, var_list=self.actor_vars)\n",
    "\n",
    "# Update values for slowly-changing targets towards current actor and critic\n",
    "tau = UPDATE_TARGET_RATE\n",
    "update_target_ops = []\n",
    "for i, target_actor_var in enumerate(target_actor_vars):\n",
    "    update_target_actor_op = target_actor_var.assign(tau * actor_vars[i] + (1 - tau) * target_actor_var)\n",
    "    update_target_ops.append(update_target_actor_op)\n",
    "\n",
    "for i, target_critic_var in enumerate(target_critic_vars):\n",
    "    update_target_critic_op = target_critic_var.assign(tau * critic_vars[i] + (1 - tau) * target_critic_var)\n",
    "    update_target_ops.append(update_target_critic_op)\n",
    "\n",
    "self.update_targets_op = tf.group(*update_target_ops, name='update_targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experience buffer is just a list to memorize experiences and sample them (we use the data structure *deque* which acts as a list with the extra ability to limit its size not to store too many experiences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "BUFFER_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class ExperienceBuffer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.buffer, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the agent with its network and its environment (like in A3C), and a run method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(self):\n",
    "\n",
    "    for ep in range(1, TRAINING_STEPS+1):\n",
    "\n",
    "        episode_reward = 0\n",
    "        episode_step = 0\n",
    "        done = False\n",
    "\n",
    "        # Initialize exploration noise process\n",
    "        noise_process = np.zeros(ACTION_SIZE)\n",
    "        noise_scale = (NOISE_SCALE_INIT * NOISE_DECAY**ep) * (HIGH_BOUND - LOW_BOUND)\n",
    "\n",
    "        # Initial state\n",
    "        s = self.env.reset()\n",
    "\n",
    "        while episode_step < MAX_EPISODE_STEPS and not done:\n",
    "\n",
    "            # choose action based on deterministic policy\n",
    "            a, = self.sess.run(self.network.actions,\n",
    "                               feed_dict={self.network.state_ph: s[None]})\n",
    "\n",
    "            # add temporally-correlated exploration noise to action\n",
    "            # (using an Ornstein-Uhlenbeck process)\n",
    "            noise_process = EXPLO_THETA * \\\n",
    "                (EXPLO_MU - noise_process) + \\\n",
    "                EXPLO_SIGMA * np.random.randn(ACTION_SIZE)\n",
    "\n",
    "            a += noise_scale * noise_process\n",
    "\n",
    "            s_, r, done, info = self.env.act(a)\n",
    "            episode_reward += r\n",
    "\n",
    "            self.buffer.add((s, a, r, s_, 0.0 if done else 1.0))\n",
    "\n",
    "            # update network weights to fit a minibatch of experience\n",
    "            if self.total_steps % TRAINING_FREQ == 0 and len(self.buffer) >= BATCH_SIZE:\n",
    "\n",
    "                minibatch = self.buffer.sample()\n",
    "                feed_dict = {self.network.state_ph: np.asarray([elem[0] for elem in minibatch]),\n",
    "                             self.network.action_ph: np.asarray([elem[1] for elem in minibatch]),\n",
    "                             self.network.reward_ph: np.asarray([elem[2] for elem in minibatch]),\n",
    "                             self.network.next_state_ph: np.asarray([elem[3] for elem in minibatch]),\n",
    "                             self.network.is_not_terminal_ph: np.asarray([elem[4] for elem in minibatch])}\n",
    "\n",
    "                _, _ = self.sess.run([self.network.critic_train_op, self.network.actor_train_op],\n",
    "                                     feed_dict=feed_dict)\n",
    "\n",
    "                # update target networks\n",
    "                _ = self.sess.run(self.network.update_slow_targets_op)\n",
    "\n",
    "            s = s_\n",
    "            episode_step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB :** In the real code, everything is wrapped up in classes and there are a few more files but it's mostly to set up the interface and get a cleaner code, the core of the algorithm has been presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "A classic environment to test DDPG is Pendulum on [Gym](http://gym.openai.com/envs/Pendulum-v0/).\n",
    "The goal here is to apply a torque on the pendulum to swing it up so it stays upright.\n",
    "Execute the cell below to see an exemple of interactions with the environment : we first apply a random torque every time step, and then just apply a counterclockwise torque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "    env.step(env.action_space.sample() if i < 100 else [2])\n",
    "    env.render()\n",
    "env.render(close=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that environment, the reward is given at each time step according to the torque applied to the pendulum, so depending on the starting point it may take a few steps to reach the top, that's why the maximum score reachable is in average around -100.\n",
    "\n",
    "We can easily test our program on that simple environment (it usually takes a few hundred episodes to stabilize around -100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./Code/DDPG/\")\n",
    "import tensorflow as tf\n",
    "from Agent import Agent\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    agent = Agent(sess)\n",
    "    agent.run()\n",
    "    agent.play(1)\n",
    "    agent.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this algorithm converges pretty quickly in that simple environment and we can visualize the final policy which is quite effective, even if on the end it fails to keep the pendulum exactly on the vertical. This is because the tiny negative reward it gets on the end by keeping a small effort on the pendulum is \"hidden\" by the large rewards from the swing at the beginning.\n",
    "\n",
    "\n",
    "![](./Images/DDPG_results.png) | ![](./Images/Pendulum.gif)\n",
    ":-:                            | :-:\n",
    "Raw and mean episode rewards   | The agent in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting continuous environment is the [Bipedal Walker](http://gym.openai.com/envs/BipedalWalker-v2/) from gym where the goal is to apply torques on each part of the legs of a simple walker to make it go as far as possible. \n",
    "From Gym documentation :\n",
    "> Reward is given for moving forward, total 300+ points up to the far end. If the robot falls, it gets -100. Applying motor torque costs a small amount of points, more optimal agent will get better score. State consists of hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, legs contact with ground, and 10 lidar rangefinder measurements. There's no coordinates in the state vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"BipedalWalker-v2\")\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "    env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "env.render(close=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This environment is more challenging because of the size of the observation space : at each time step, the agent receives 24 different values (instead of 3 for the Pendulum).\n",
    "\n",
    "We can run the same algorithm to solve this environment, except it will take more time to achieve satisfying results. I ran it for about 6 hours \n",
    "\n",
    "\n",
    "\n",
    "Results after 4h                  | Results after 6h\n",
    ":-:                               | :-:\n",
    "![](./Images/BipedalWalker_1.gif) | ![](./Images/BipedalWalker_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* A possible amelioration is to upgrade the experience buffer to a prioritized one [[Schaul et al.](https://arxiv.org/pdf/1511.05952.pdf)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainbow DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept\n",
    "Since the apparition of DQN in 2013, many improvements have been developped to enhance the basic algorithm such as Double DQN or C51, but no one had tried to combine all these ameliorations until October 2017 giving the Rainbow algorithm [[Hessel et al.](https://arxiv.org/pdf/1710.02298.pdf)].\n",
    "\n",
    "Five main improvements were used in that algorithm :\n",
    "- Double DQN [[Van Hasselt et al.](https://arxiv.org/pdf/1509.06461.pdf)]\n",
    "- Dueling DQN [[Wang et al.](https://arxiv.org/pdf/1511.06581.pdf)]\n",
    "- Prioritized Experience Replay [[Schaul et al.](https://arxiv.org/pdf/1511.05952.pdf)]\n",
    "- Distributional DQN [[Bellemare et al.](https://arxiv.org/abs/1707.06887)]\n",
    "- Noisy DQN [[Fortunato et al.](https://arxiv.org/pdf/1706.10295.pdf)]\n",
    "\n",
    "![](./Images/Rainbow.jpg)\n",
    "\n",
    "I won't describe all these enhancements in details since a better description is given on the [related blog post](https://rlsupaero.wordpress.com/2017/11/08/deep-rl-key-papers/) (along other approaches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "*This code can be found in ./BlogFiles/Code/Rainbow/*\n",
    "\n",
    "\n",
    "The benefit of this algorithm is that it doesn't require a whole new architecture, we can just start from a classical DQN implementation and add features on top of it. That's what I did with the double and the dueling DQN, and with PER but I didn't have time to add Distributional and Noisy DQN yet.\n",
    "\n",
    "The implementation is quite simple and the general outline of the code is exactly the same as the other codes (A3C and DDPG). As usual, let's analyze the code for the network architecture and the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, let's build a dueling convolutional network. The network is composed of three convolutional layers followed by two streams of fully connected layers that output the advantage of each action (ACTION_SIZE outputs) and the value of the input state (1 output).\n",
    "\n",
    "![](./Images/dueling_dqn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    inputs = tf.placeholder(tf.float32, [None, *STATE_SIZE],\n",
    "                                 name='Input_state')\n",
    "\n",
    "    with tf.variable_scope('Convolutional_Layers'):\n",
    "        conv1 = tf.layers.conv2d(inputs=inputs,\n",
    "                                 filters=32,\n",
    "                                 kernel_size=[8, 8],\n",
    "                                 strides=[4, 4],\n",
    "                                 padding='VALID',\n",
    "                                 activation=tf.nn.relu)\n",
    "        conv2 = tf.layers.conv2d(inputs=conv1,\n",
    "                                 filters=64,\n",
    "                                 kernel_size=[4, 4],\n",
    "                                 strides=[2, 2],\n",
    "                                 padding='VALID',\n",
    "                                 activation=tf.nn.relu)\n",
    "        conv3 = tf.layers.conv2d(inputs=conv2,\n",
    "                                 filters=64,\n",
    "                                 kernel_size=[3, 3],\n",
    "                                 strides=[1, 1],\n",
    "                                 padding='VALID',\n",
    "                                 activation=tf.nn.relu)\n",
    "\n",
    "        # Flatten the output\n",
    "        hidden = tf.layers.flatten(conv3)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def dueling(hidden):\n",
    "\n",
    "    advantage_stream = tf.dense(hidden, 32, activation=tf.nn.relu)\n",
    "    value_stream = tf.dense(hidden, 32, activation=tf.nn.relu)\n",
    "\n",
    "    advantage = tf.dense(advantage_stream, ACTION_SIZE, activation=None)\n",
    "    value = tf.dense(value_stream, 1, activation=None)\n",
    "\n",
    "    return value, advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the network which combines these two outputs to get the Q-Value and define the operations to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Network\"):\n",
    "\n",
    "    # Define the model as above (wrapped in a class)\n",
    "    model = NetworkArchitecture(STATE_SIZE, ACTION_SIZE)\n",
    "\n",
    "    # Convolution network\n",
    "    inputs = model.build_model()\n",
    "\n",
    "    # Dueling DQN\n",
    "    value, advantage = model.dueling()\n",
    "\n",
    "    # Aggregation of value and advantage to get the Q-value\n",
    "    adv_mean = tf.reduce_mean(advantage, axis=1, keep_dims=True)\n",
    "    Qvalues = value + tf.subtract(advantage, adv_mean)\n",
    "\n",
    "    predict = tf.argmax(Qvalues, 1)\n",
    "\n",
    "    # Loss\n",
    "    Qtarget = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "    actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "    actions_onehot = tf.one_hot(actions, ACTION_SIZE, dtype=tf.float32)\n",
    "\n",
    "    Qaction = tf.reduce_sum(tf.multiply(Qvalues, actions_onehot), axis=1)\n",
    "\n",
    "    td_error = tf.square(Qtarget - Qaction)\n",
    "    loss = tf.reduce_mean(td_error)\n",
    "    \n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    train = trainer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define an agent with a main network, a target network and a Prioritized Replay Buffer.\n",
    "For the PER, I took the [OpenAI implementation](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py) from their `baselines` algo bank that implements the right data structure (a SumTree, *cf* the original PER paper for more information) and allows to add experiences and retrieve them efficiently and quickly.<br/>\n",
    "The main methods from the PrioritizedReplayBuffer structure are :\n",
    "- add(s, a, r, s', terminal)\n",
    "- sample(batch_size, beta) : samples `batch_size` episodes with `beta` the importance-sampling parameter ($\\beta = 0 \\implies$no prioritization;  $\\beta = 1 \\implies$ strong prioritization)\n",
    "- update_priorities(indexes, weights)\n",
    "\n",
    "During the initialization of a PrioritizedReplayBuffer, the $\\alpha$ parameter must be given to determine how much prioritization is used.\n",
    "\n",
    "Additionnally, we implement N-step learning : instead of using a single reward as a target, we wait for multi-steps to get a better estimation of the reward and get a new loss $$[\\sum_{k=0}^{n-1}{\\gamma^k r_{t+k}} + \\gamma^n \\max_a{Q(s_{t+n}, a)} - Q(s_t, a_t)]^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, sess):\n",
    "        \n",
    "        [...]\n",
    "        self.env = Environment()\n",
    "\n",
    "        self.mainQNetwork = QNetwork(STATE_SIZE, ACTION_SIZE, 'main')\n",
    "        self.targetQNetwork = QNetwork(STATE_SIZE, ACTION_SIZE, 'target')\n",
    "        \n",
    "        self.buffer = PrioritizedReplayBuffer(BUFFER_SIZE, ALPHA)\n",
    "        [...]\n",
    "        \n",
    "    def run(self):\n",
    "\n",
    "        self.nb_ep = 1\n",
    "\n",
    "        while self.nb_ep < TRAINING_STEPS:\n",
    "\n",
    "            s = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_step = 0\n",
    "            done = False\n",
    "\n",
    "            memory = deque()\n",
    "\n",
    "            while episode_step < MAX_EPISODE_STEPS and not done:\n",
    "\n",
    "                # Epsilon-greedy policy\n",
    "                if random.random() < self.epsilon:\n",
    "                    a = random.randint(0, self.action_size - 1)\n",
    "                else:\n",
    "                    a = self.sess.run(self.mainQNetwork.predict,\n",
    "                                      feed_dict={self.mainQNetwork.inputs: [s]})\n",
    "                    a = a[0]\n",
    "\n",
    "                s_, r, done, info = self.env.act(a)\n",
    "                episode_reward += r\n",
    "\n",
    "                memory.append((s, a, r, s_, done))\n",
    "\n",
    "                # We have enough experience in the memory to compute the N-Step reward\n",
    "                if len(memory) > N_STEP_RETURN:\n",
    "                    s_mem, a_mem, r_mem, ss_mem, done_mem = memory.popleft()\n",
    "                    discount_R = r_mem\n",
    "                    for i, (si, ai, ri, s_i, di) in enumerate(memory):\n",
    "                        discount_R += ri * GAMMA ** (i+1)\n",
    "                    self.buffer.add(s_mem, a_mem, discount_R, s_, done)\n",
    "\n",
    "                if episode_step % TRAINING_FREQ == 0:\n",
    "\n",
    "                    train_batch = self.buffer.sample(BATCH_SIZE, self.beta)\n",
    "\n",
    "                    # Incr beta\n",
    "                    if self.beta <= BETA_STOP:\n",
    "                        self.beta += BETA_INCR\n",
    "\n",
    "                    # We compute the old Q-Values of the (state, action) pair\n",
    "                    # to update the weights of our prioritized buffer\n",
    "                    feed_dict = {self.mainQNetwork.inputs: train_batch[0]}\n",
    "                    oldQvalues = self.sess.run(self.mainQNetwork.Qvalues,\n",
    "                                               feed_dict=feed_dict)\n",
    "                    tmp = [0] * len(oldQvalues)\n",
    "                    for i, oldQvalue in enumerate(oldQvalues):\n",
    "                        tmp[i] = oldQvalue[train_batch[1][i]]\n",
    "                    oldQvalues = tmp\n",
    "\n",
    "                    feed_dict = {self.mainQNetwork.inputs: train_batch[3]}\n",
    "                    mainQaction = self.sess.run(self.mainQNetwork.predict,\n",
    "                                                feed_dict=feed_dict)\n",
    "\n",
    "                    feed_dict = {self.targetQNetwork.inputs: train_batch[3]}\n",
    "                    targetQvalues = self.sess.run(self.targetQNetwork.Qvalues,\n",
    "                                                  feed_dict=feed_dict)\n",
    "\n",
    "                    done_multiplier = (1 - train_batch[4])\n",
    "                    doubleQ = targetQvalues[range(BATCH_SIZE), mainQaction]\n",
    "                    targetQvalues = train_batch[2] + GAMMA * doubleQ * done_multiplier\n",
    "\n",
    "                    errors = np.square(targetQvalues - oldQvalues) + 1e-6 # to be sure != 0\n",
    "                    self.buffer.update_priorities(train_batch[6], errors)\n",
    "\n",
    "                    feed_dict = {self.mainQNetwork.inputs: train_batch[0],\n",
    "                                 self.mainQNetwork.Qtarget: targetQvalues,\n",
    "                                 self.mainQNetwork.actions: train_batch[1]}\n",
    "                    _ = self.sess.run(self.mainQNetwork.train,\n",
    "                                      feed_dict=feed_dict)\n",
    "\n",
    "                    update_target(self.update_target_ops, self.sess)\n",
    "\n",
    "                s = s_\n",
    "                episode_step += 1\n",
    "\n",
    "            self.nb_ep += 1\n",
    "\n",
    "            # Decay epsilon\n",
    "            if self.epsilon > EPSILON_STOP:\n",
    "                self.epsilon -= EPSILON_DECAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Another very classic example of environment is [CartPole](http://gym.openai.com/envs/CartPole-v0/) on gym :\n",
    "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "Let's visualize it by going left and right alternatively :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "for i in range(-15, 150):\n",
    "    env.step((i//30)%2)\n",
    "    env.render()\n",
    "env.render(close=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can try to solve this environment with our program :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mpi4py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-13d59bfff7d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mAgent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ingenieurs18/v.guillet/Documents/Cesure/GitRepository/RL-Agents/BlogFiles/Code/Rainbow/Agent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mEnvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrioritizedReplayBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mQNetwork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ingenieurs18/v.guillet/libs/baselines/baselines/deepq/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_train\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrioritizedReplayBuffer\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ingenieurs18/v.guillet/libs/baselines/baselines/deepq/simple.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSchedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ingenieurs18/v.guillet/libs/baselines/baselines/logger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpi4py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mLOG_OUTPUT_FORMATS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'stdout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpi4py'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:os.chdir(\"./Code/Rainbow/\")\n",
    "except FileNotFoundError:pass\n",
    "import tensorflow as tf\n",
    "from Agent import Agent\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    agent = Agent(sess)\n",
    "    agent.run()\n",
    "    agent.play(1)\n",
    "    agent.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning is quite fast (two minutes at most) due to the simplicity of the environment and the final policy is very effective, reaching the highest possible score almost every time. The few runs where it doesn't reach the highest score are due to the $\\epsilon$-greedy exploration policy which is still quite important before 1000 episodes.\n",
    "\n",
    "![](./Images/Rainbow_results.png) | ![](./Images/CartPole.gif)\n",
    ":-:                               | :-:\n",
    "Raw and mean episode rewards      | The agent in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to sove another environment [Lunar Lander](http://gym.openai.com/envs/LunarLander-v2/) where the goal is to land a lunar module on a pad by activating its motors.\n",
    "> Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-23 17:56:40,818] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "for i in range(120):\n",
    "    env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "env.render(close=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the Bipedal Walker environment, the computation is way longer than for CartPole (a few hours) but the end results are pretty satisfying\n",
    "\n",
    "![](./Images/LunarLander_results.png) | ![](./Images/LunarLander.gif)\n",
    ":-:                                   | :-:\n",
    "Mean episode rewards          | The agent in action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
