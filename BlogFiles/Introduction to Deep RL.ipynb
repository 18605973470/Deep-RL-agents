{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Reinforcement Learning *via* the Arcade Learning Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Background\n",
    "A quick recap on reinforcement learning and the notations we will adopt here : we address the problem of an **agent** learning to act in an **environment** in order to maximize a **reward** signal.\n",
    "\n",
    "\n",
    "At each time step, the environment provides an observation $s_t \\in S$, the agent responds by selecting an action $a_t \\in A$ and then the environment provides a reward $r_{t+1}$ and the next state $s_{t+1}$ with a discount factor $\\gamma \\in [0, 1]$.\n",
    "We assume that this system is a Markov Decision Process e.g. the state of the system at the time step $t+1$ only depends on the state and the action chose at the time step $t$.\n",
    "\n",
    "\n",
    "The goal of RL is to find a policy $\\pi$ (e.g. a probability distribution over actions for each state) that maximize the expected discounted return $G_t = \\sum_{k=0}^{+\\infty}{\\gamma^k R_{t+k+1}}$ over an episode in the environment.\n",
    "\n",
    "To do so, we learn an estimate of the **Q-Value function** $Q_\\pi(s_t, a_t) = E_\\pi[G_t|s_t, a_t]$ which is equal to the expected reward an agent will receive starting from a state $s_t$ and action $a_t$ and then acting under policy $\\pi$. Once this Q-Value function is known, an optimal policy is trivial : $\\pi(s_t) = max_{a \\in A}{Q(s_t, a)}$.\n",
    "\n",
    "Let's also define the **Value function** $V_\\pi(s_t) = E_\\pi[G_t|s_t]$ the expected discounted reward obtained following a policy $\\pi$ starting from a state $s_t$.\n",
    "\n",
    "\n",
    "*For more details, see the [wikipedia page on MDPs][wikiMDP].*\n",
    "\n",
    "\n",
    "\n",
    "### DQN Algorithm\n",
    "Theorically, some methods exist to compute the Q-Value function, such as tabular Q-Learning. The idea is to start from a random guess of the Q-Value of each pair (state, action) and iteratively apply [Bellman Equation][BellEq] until it converges toward the real value of the function *(see [this post by Arthur Juliani][JulianiPost] to get an example)*.\n",
    "\n",
    "\n",
    "But this method is only appliable on very few problems with tiny state and action spaces, and usual problems generally have nearly infinite large state space, and even if the idea is not bad, the tabular approach cannot work on them.\n",
    "\n",
    "\n",
    "Deep Reinforcement Learning is an efficient method to solve such problems. The idea is to approximate the Q-Value function with a deep neural network trained by gradient descent to then derive a well performing policy from it.\n",
    "It was successfully implemented for the first time in 2015 [[Mnih et al]][DQN] as DQN by combining the use of convolutional nets to efficiently processes the raw frames fed as input to the network and of a replay memory buffer not to learn only on immediate rewards.\n",
    "\n",
    "\n",
    "The optimization by gradient descent is realized wrt to the loss on a randomly chosen time step picked from the replay memory $$(R_t + \\gamma_{t+1} max_aQ_\\bar{\\theta}(s_{t+1}, a) - Q\\theta(s_t, a_t))^2$$ with $Q_\\bar{\\theta}$ the *online network* (the network used to select the action) and $Q_\\theta$ the *target network* (a copy of the network periodically updated to stabilize the learning).\n",
    "\n",
    "\n",
    "DQN was the first concrete example of successful reinforcement learning algorithm, and it opened the way to many researches and improvements since 2015. In this notebook, I will present three modified versions of DQN that has lead to improvements or that extended the application domain :\n",
    "* A3C : an asynchronous version that allows parallelism\n",
    "* DDPG : a continuous version\n",
    "* Rainbow DQN : an improved version of DQN, one of the most efficient algorithm of Deep RL today\n",
    "\n",
    "\n",
    "[wikiMDP]: https://en.wikipedia.org/wiki/Markov_decision_process\n",
    "[BellEq]: https://en.wikipedia.org/wiki/Bellman_equation#Example\n",
    "[JulianiPost]: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "[DQN]: https://www.nature.com/articles/nature14236"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Advantage Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept\n",
    "Presented in 2016 [[Mnih et al. again]][A3C], A3C is quite different from regular DQN : it's an Actor-Critic. First, let's debunk the name of the algorithm to understand it better :\n",
    "+ **Asynchronous :** Asynchronous algorithm is a great family of algos designed to run on parallel architecture to speed the learning up. The main idea is instead of having only one agent interacting with an environment, an asynchronous algo creates a global network and many workers on parallel threads with each their own environment and their own neural network. Then, during an episode, each agent copies the weights of the global network, then interacts with it's own environment and apply a gradient descent on the weight of the global network.\n",
    "\n",
    " This has two main advantages :\n",
    "  - the parallelism of today hardware allows to run tens or even hundreds of workers on different thread, and so to maximize the work done in a given time\n",
    "  - each worker being independent of the others, they collect a lot of various experience and assure a better exploration of the state space, removing the necessity to have a replay memory buffer\n",
    "\n",
    "\n",
    "+ **Actor-Critic :** Instead of just estimating the Q-Value function and then inducing a policy by acting greedily with respect to the action Q-Values, in Actor-Critic algorithms, the network estimate both the Value $V$ function and a policy $\\pi$. The value estimator is called the **critic** and the policy estimator the **actor**.\n",
    "\n",
    "\n",
    "\n",
    "+ **Advantage :** We define advantage as the difference between the Q-Value and the Value of a given state and action $A(s, a) = Q(s, a) - V(s)$ : it represents how much better an action is than expected. During a gradient descent, the updates usually use discounted reward to tell the quality of a taken action, but one way to be more efficient is instead to use an advantage estimate to get how much better it is than average.\n",
    "\n",
    " We can estimate this advantage quite easily because $V(s_t)$ is the output of our network and $Q(s_t, a_t)$ can be estimated by the discounted reward $R_t = \\sum_{k=0}^{+\\infty}{\\gamma^k r_{t+k}}$ : $$A_{est}(s_t, a_t) = R_t - V(s_t)$$\n",
    " \n",
    "![alt text](./Images/A3C.png \"Architecture of an A3C Network\")\n",
    "\n",
    "### Implementation\n",
    "*This code can be found in ../A3C/Discrete*\n",
    "\n",
    "Let's now see an actual implementation of the A3C algorithm applied to a simple Atari 2600 game : Pong.\n",
    "First, the general outline of the code architecture :\n",
    "- **Network.py** : defines a class Network that contains the neural network definition (convolution + LSTM layers), the Tensorflow operations to compute and apply gradients on the network weights and the operations to copy the weights of the global network to the local one\n",
    "- **Agent.py** : defines a class Agent that contains a local network and a local environment and who can run episodes to get experience and apply gradient descents to the global network\n",
    "- **main.py** : the main program that creates a global network and many workers, and run them on different threads\n",
    "- **settings.py** : contains every important setting that can be modified in the algorithm\n",
    "\n",
    "\n",
    "- **Environment.py** is a wrapper for [ALE](https://github.com/mgbellemare/Arcade-Learning-Environment) Pong game\n",
    "- **RMSPropApplier.py** is a modified version of tensorflow optimizer that can be shared between every worker like in the original paper of A3C (code taken from [Miyosuda implentation](https://github.com/miyosuda/async_deep_reinforce))\n",
    "- **Displayer.py**, **Saver.py**, **eval.py** : defines visualisation, saving and testing tools\n",
    "\n",
    "Now, let's analyze more precisely the main lines of the code.\n",
    "\n",
    "[A3C]: https://arxiv.org/pdf/1602.01783.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's build the neural network "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
