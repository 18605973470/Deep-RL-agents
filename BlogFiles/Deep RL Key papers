<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title></title>
</head>
<body>

[Post rédigé à deux mains par Valentin et Emmanuel]
<h3>Foundation:</h3>

<ul><li>
<h4><h4>DQN</h4></h4>
Key paper : https://arxiv.org/abs/1312.5602
Summary : Q-learning with a CNN as value function. Mini-batches of samples. Target network to stabilize the learning.
</li></ul>


<h3>Extensions:</h3>

<ul><li>
<h4>A3C (Asynchronous Advantage Actor-Critic)</h4>
Key paper : https://arxiv.org/abs/1602.01783
Summary : Really efficient asynchronous (=many workers learning on different threads) actor-critic (=network that learns Policy and Value) algo, use an LSTM layer for temporal dependencies.
</li></ul>

<hr />

<ul><li>
<h4>DDPG (Deep Deterministic Policy Gradient)</h4>
Key paper : https://arxiv.org/pdf/1509.02971.pdf
Summary : Trying to address continuous action space : Deterministic PG introduced in 2014 [http://proceedings.mlr.press/v32/silver14.pdf], combined with DQN. This gives a model-free actor-critic that directly outputs Q-value and an action.
</li></ul>

<hr />

<ul><li>
<h4>Prioritized experience replay</h4>
Key paper : https://arxiv.org/abs/1511.05952
Summary : Not all experiences bring the same amount of information : when TD-Error is large, this means that the network wasn't expecting that e.g we can learn a lot from this experience => prioritization (more chance to be replayed)
</li></ul>

<hr />

<ul><li>
<h4>Double DQN</h4>
Key paper : https://arxiv.org/abs/1509.06461
Summary : A regular DQN only chooses the actions that he considers as the best leading to tendancy to overestimate the real Q-value, which can destabilize the learning. To avoid that, we create two networks : one to choose actions and the other one to evaluate the Q-value of this action.
</li></ul>

<hr />

<ul><li>
<h4>C51</h4>
Key paper : https://arxiv.org/abs/1707.06887
Summary : Keeping the knowledge of the distribution of values and taking the average at each Q-learning iteration is more efficient when the Q-function is approximated.
Extensions or related : https://arxiv.org/abs/1710.10044 https://openreview.net/forum?id=SyZipzbCb
</li></ul>

<hr />

<ul><li>
<h4>NoisyNets</h4>
Key paper : https://arxiv.org/abs/1706.10295
Summary : Instead of using a classic exploration policy such as epsilon greedy, we apply random Gaussian noise over the weights and biases of the network to smoothly randomize the action choice.
</li></ul>

<hr />

<ul><li>
<h4>FiGAR</h4>
Key paper : https://arxiv.org/abs/1702.06054
Summary : Choosing an action at each time step prevents the network to develop multi-time-step strategies (swim up during 20 frames to get out of the sea and breathe in Seaquest for example). To avoid that, in FiGAR, the network is modified to predict an action (as usual) but also the number of times this action will be repeated => temporal abstraction.
Extensions or related : https://arxiv.org/abs/1605.05365
</li></ul>

<hr />

<ul><li>
<h4>N-Step returns</h4>
Key paper :
Summary : 
Extensions or related :
</li></ul>

<hr />

<ul><li>
<h4>Dueling DQN (multi-target DQN)</h4>
Key paper : https://arxiv.org/abs/1511.06581
Summary : Instead of just computing the Q-value of an action, it can be interesting to compute the value of the state we're in and the advantage of this particular action. This allows the network to make the difference between a good general state of the system independant of the policy, and an average state where the network has to come up with the right action.
</li></ul>

<hr />

<ul><li>
<h4>Using RNNs</h4>
Key paper : https://arxiv.org/pdf/1507.06527.pdf
Summary : Introduction of LSTM cells into a DQN to solve Partially Observable MDP
</li></ul>

<hr />

<ul><li>
<h4>Deep energy-based policies</h4>
Key paper : https://arxiv.org/abs/1702.08165 http://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/
Summary :
Extensions or related :
</li></ul>

<hr />

<ul><li>
<h4>TRPO (Trust Region Policy Optimization)</h4>
Key paper : https://arxiv.org/abs/1502.05477
Summary :
Extensions or related :
</li></ul>
<h3>Comparisons of algos:</h3>
<ul><li>
<h4>Rainbow</h4>
Key paper : https://arxiv.org/abs/1710.02298
Summary :
</li></ul>
<h3><strong>Robotics :</strong></h3>
<ul><li>
<h4>Transfering from simu to real-world</h4>
https://arxiv.org/abs/1710.06542
</li></ul>

<h3><strong>Approches alternatives moins populaires :</strong></h3>

<ul>
    <li>Generalized advantage learning : https://arxiv.org/abs/1506.02438</li>
    <li>Combining Policy Gradient and Q-learning : https://arxiv.org/abs/1611.01626</li>
    <li>Batch normalization : https://arxiv.org/abs/1502.03167</li>
</ul>
 




</body>
</html>
